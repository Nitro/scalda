332 Hormel 
A $elt-organlzlng Associative 
Memory System tot Control 
Applications 
Michael Hormel 
Department of Control Theory and Robotics 
Technical University of Darmstadt 
$chlossgraben 1 
6100 Darmstadt/W.-Germany 
ABSTRACT 
The CMAC storage scheme has been used as a basis 
for a software implementation of an associative 
memory system AMS, which itself is a major part 
of the learning control loop LENAS. A major 
disadvantage of this eMAC-concept is that the 
degree of local generalization (area of interpo- 
lation) is fixed. This paper deals with an algo- 
rithm for self-organizing variable generaliza- 
tion for the AMS, based on ideas of T. Kohonen. 
I INTRODUCTION 
For several years research at the Department of Control The- 
ory and Robotics at the Technical University of Darmstadt 
has been concerned with the design of a learning real-time 
control loop with neuron-like associative memories (LERNAS) 
A Self-organizing Associative Memory System for Control Applications 333 
for the control of unknown, nonlinear processes (Ersue, 
Tolle, 1988). This control concept uses an associative memo- 
ry system AMS, based on the cerebellar cortex model CMAC by 
AIbus (Albus, 1972), for the storage of a predictive nonlin- 
ear process model and an appropriate nonlinear control stra- 
tegy (Fig. 1). 
expected process response 
evaluation/ 
optimization 
optimized 
control input 
r- control sirall 
des:red setpoint ' 
process inforeation 
planne control inputs 
process model 
actual/past 
I process information 
unknorn process 
short term 
me mo ry 
l associative BeBory system AHS 
Figure 1: The learning control loop hERNAS 
One problem for adjusting the control loop to a process is, 
however, to find a suitable set of parameters for the asso- 
ciative memory. The parameters in question determine the 
degree of generalization within the memory and therefore 
have a direct influence on the number of training steps re- 
quired to learn the process behaviour. For a good perfor- 
mance of the control loop it-is desirable to have a very 
small generalization around a given setpoint but to have a 
large generalization elsewhere. Actually, the amount of col- 
lected data is small during the transition phase between two 
334 Hormel 
setpoints but is large during setpoint control. Therefore a 
self-organizing variable generalization, adapting itself to 
the amount of available data would be very advantageous. 
Up to now, when working with fixed generalization, finding 
the right parameters has meant to find the best compromise 
between performance and learning time required to generate a 
process model. This paper will show a possibility to intro- 
duce a self-organizing variable generalization capability 
into the existing AIi$/CMAC algorithm. 
2 THE AMS-CONC�PT 
The associative memory system AHS is based on the Cerebel- 
lar Hodel lrticulation Controller CHA� as presented by J.S. 
Albus. The information processing structure of AIi$ can be 
divided into three stages. 
1.) 
Each component of a n-dimensional input vector (stimu- 
lus) activates a fixed number p of sensory cells, the 
receptive fields of which are overlapping. go n-p sen- 
sory cells become active. 
cells described by one 
in the n-dimensional 
receptive field of the 
plications the total 
cells is about 100.p. 
2.) The active sensory cells are grouped to form p n-dimen- 
sional vectors. These vectors are mapped to � associa- 
tion cells. The merged receptive fields of the sensory 
vector can be seen as a hypercube 
input space and therefore as the 
association cell. In normal ap- 
number of available association 
3.) The association cells are connected to the output cells 
by modifiable synaptic weights. The output cell computes 
the mean value of all weights that are connected to ac- 
tive association cells (active weights). 
Figure 2 shows the basic principle of the associative memory 
system A]4$. 
A Self-organizing Associative Memory System for Control Applications 335 
S l 
input space 
output value 
adjustable weigh 
Figure 2: The basic mechanism of 
During training the generated output is compared with a de- 
sired output, the error is computed and equally distributed 
over all active weights. For the mapping of sensory cells to 
association cells a hash-coding mechanism is used. 
3 THE S�LF-ORGANIZING F�ATUR� MAP 
An approach for explaining the self-organizing 
of the nervous system has been presented by T. 
honen, 1988). 
capabilities 
Kohonen (Ko- 
In his self-organizing feature map a network of laterally 
interconnected neurons can adapt itself according to the 
density of trained points in the input space. Presenting a 
n-dimensional input vector to the network causes every neu- 
ron to produce an output signal which is correlated with the 
similarity between the input vector and a template vector 
which may be stored in the synaptic weights of the neuron. 
Due to the mexican-hat coupling function between the neu- 
rons, the one with the maximum output activity will excite 
its nearest neighbouts but will inhibit neurons farther a- 
way, therefore generating a localized response in the net- 
work. The active cells can now adapt their input weights in 
order to increase their similarity to the input vector. If 
we define the receptive field of a neuron by the number of 
input vectors for which the neurons activity is greater than 
336 Hormel 
that of any other neuron in the net, this yields the effect 
that in areas with a high density of trained points the re- 
ceptive fields become small whereas in areas with a low den- 
sity of trained points the size of the receptive fields is 
large. As mentioned above this is a desired effect when 
working with a learning control loop. 
4 SELF-ORGANIZING VARIABLE GENERALIZATION 
Both of the approaches above have several advantages and 
disadvantages when using them for real-time control applica- 
tions. 
In the M$ algorithm one does not have to care for prede- 
fining a network and the coupling functions or coupling ma- 
trices among the elements of the network. Association and 
weight cells are generated when they are needed during 
training and can be adressed very quickly to produce a memo- 
ry response. One of the disadvantages is the fixed generali- 
zation once the parameters of a melory unit have been 
chosen. 
Unlike M$, the feature map allows the adaption of the net- 
work according to the input data. This advantage has to be 
payed for by extensive search for the best matching neuron 
in the network and therefore the response time of the net- 
work may be too large for real-tile control when working 
with big networks. 
These problems can be overcome when allowing that the 
ping of sensory cells to association cells in AMS 
longer fixed but can be changed during training. 
iccp- 
is no 
To accomplish this a template vector [ is introduced for 
every association cell. This vector [ serves as an indicator 
for the stimuli by which the association cell has been ac- 
cessed previously. During an associative recall for a stimu- 
lus 0 a preliminary set of p association cells is activated 
by the hash coding mechanism. Due to the self-organizing 
process during training the template vectors do not need to 
correspond to the input vector 0' For the search for the 
A Self-organizing Associative Memory System for Control Applications 337 
best matching cell the template vector 0 of the 
association cell is compared to the stimulus and a 
ence vector is calculated. 
accessed 
differ- 
4. = t. - 0 i = O,...,n 
(1) 
n number of searching steps 
This vector can 
which compensates 
mechanism. 
now be used to compute a 
the mapping errors of 
virtual stimulus 
the hash-coding 
i = O,...,n 
(2) 
The best matching cell 
a -- II & II 
i 
is found for 
i = O, ... ,n s 
(3) 
and can be adressed by the virtual stimulus Ej when using 
the bash coding mechanism. This search mechanism ensures 
that the best matching cell is found even if self organiza- 
tion is in effect. 
During training the template 
cells are updated by 
vectors of the 
d network 
association 
t_(k+l) = (k,d).(s(k) -t_(k)) + t_(k) (4) 
lateral distance of neurons in the 
where [(k) denotes the value of the template vector at time 
k and [(k) denotes the stimulus. e(k,d) is a monotonic de- 
creasing function of time and the lateral distance between 
neurons in the network. 
5 SIMULATION RESULTS 
Figure 3 and 4 show some simulation results of the presented 
algorithm for the dase of a two dimensional stimulus vector. 
338 Hormel 
Figure 3 shows the expected positions in input space of the 
untrained template vectors ( x denotes untrained association 
cells). 
Figure 3: Untrained Network 
Figure 4 shows the network after 2000 training steps with 
stimuli of gaussian distribution in input space. The posi- 
tion of the template vectors of trained cells has shifted 
into the direction of the better trained areas, so that more 
association cells are used to represent this area than be- 
fore. Therefore the stored information will be more exact in 
this area. 
Figure 4: Network after 2000 training steps 
A Self-organizing Associative Memory System for Control Applications 339 
6 CONCLUSION 
The new algorithm presented above introduces the capability 
to adapt the storage mechanisms of a CMA�-type associative 
memory according to the arriving stimuli. This will result 
in various degrees of generalization depending on the number 
of trained points in a given area. It therefore will make it 
unnecessary to choose a generalization factor as a compro- 
mise between several constraints when representing nonlinear 
functions by storing them in this type of associative memo- 
ry. Some results on tests will be presented together with a 
comparison on respective results for the original AMS. 
Acknowledgements 
This work was sponsored by the German Ministry for 
and Technology (BMFT) under grant no. ITR 8800 B/5 
Research 
References 
E. Ersue, H
