348 Farotimi, Dembo and Kailath 
Neural Network Weight Matrix Synthesis 
Optimal Control Techniques 
Using 
O. Farotimi 
A. Dembo 
Information Systems Lab. 
Electrical Engineering Dept. 
Stanford University, 
Stanford, CA 94305 
T. Kailath 
ABSTRACT 
Given a set of input-output training samples, we describe a proce- 
dure for determining the time sequence of weights for a dynamic 
neural network to model an arbitrary input-output process. We 
formulate the input-output mapping problem as an optimal con- 
trol problem, defining a performance index to be minimized as a 
function of time-varying weights. We solve the resulting nonlin- 
ear two-point-boundary-value problem, and this yields the training 
rule. For the performance index chosen, this rule turns out to be a 
continuous time generalization of the outer product rule earlier sug- 
gested heuristically by Hopfield for designing associative memories. 
Learning curves for the new technique are presented. 
I INTRODUCTION 
Suppose that we desire to model as best as possible some unknown map qb � L/ --, 
12, where H, 12 C_ '. One way we might go about doing this is to collect as many 
input-output samples {(9i,, o,t) ' qb(i,) = 9o, t } as possible and find some func- 
tion f' H --, 12 such that a suitable distance metric d(f(z(t)), qb(z(t)))lzE(t),,,:qb(t,,)=t)o.,} 
is minimized. 
In the foregoing, we assume a system of ordinary differential equations motivated by 
dynamic neural network structures[i] [2]. In particular we set up an n-dimensional 
Neural Network Weight Matrix Synthesis 349 
neural network; call it N'. Our goal is to synthesize a possibly time varying weight 
matrix for A/' such that for initial conditions z(t0), the input-output transformation, 
or flow f: z(t0) -- f(z(tl) ) associated with A/' approximates closely the desired 
map b. 
For the purposes of synthesizing the weight program for A/', we consider another sys- 
tem, say $, a formal nL-dimensional system of differential equations comprising L 
n-dimensional subsystems. With the exception that all L n-dimensional subsystems 
are constrained to have the same weight matriz, they are otherwise identical and 
decoupled. We shall use this system to determine the optimal weight program given 
L input-output samples. The resulting time program of weights is then applied to 
the original n-dimensional system A/' during normal operation. We emphasize the 
difference between this scheme and a simple L-fold replication of A/': the latter 
will yield a practically unwieldy nL x nL weight matrix sequence, and in fact will 
generally not discover the underlying map from L/to 2, discovering instead differ- 
ent maps for each input-output sample pair. By constraining the weight matrix 
sequence to be an identical n x n matrix for each subsystem during this synthesis 
phase, our scheme in essence forces the weight sequence to capture some underlying 
relationship between all the input-output pairs. This is arguably the best estimate 
of the map given the information we have. 
Using formal optimal control techniques[3], we set up a performance index to max- 
imize the correlation between the system $ output and the desired output. This 
optimization technique leads in general to a nonlinear two-point-boundary-value 
problem, and is not usually solvable analytically. For this particular performance 
index we are able to derive an analytical solution to the optimization problem. 
The optimal interconnection matrix at each time is the sum (over the index of all 
samples) of the outer products between each desired output n-vector and the cor- 
responding subsystem output. At the end of this synthesis procedure, the weight 
matrix sequence represents an optimal time-varying program for the weights of the 
n-dimensional neural network A/' that will approximate b :L/-- 2. 
We remark that in the ideal case, the weight matrix at the final time (i.e one element 
of the time sequence) corresponds to the symmetric matrix suggested empirically by 
Hopfield for associative memory applications[4]. It becomes clear that the Hopfield 
matrix is suboptimal for associative memory, being just one point on the optimal 
weight trajectory; it is optimal only in the special case where the initial conditions 
coincide exactly with the desired output. 
In Section 2 we outline the mathematical formulation and solution of the synthesis 
technique, and in Section 3 we present the learning curves. The learning curves also 
by default yield the system performance over the training samples, and we compare 
this performance to that of the outer product rule. In Section 4 we give concluding 
remarks and give the directions of our future work. 
Although the results here are derived for a specific case of the neuron state equation, 
and a specific choice of performance index, in further work we have extended the 
results to very general state equations and performance indices. 
350 Farotimi, Dembo and Kailath 
2 SYNTHESIS OF WEIGHT MATRIX TIME SEQUENCE 
Suppose we have a training set consisting of L pairs of n-dimensional vectors 
(t(r)i, 0(r)), r = 1, 2,..., L, i = 1, 2,..., n. For example, in an autoassociative sys- 
tem in which we desire to store 0(r), r = 1,2,..., L, i = 1,2,..., n, we can choose 
the O(r)i,r -- 1,2,...,L,i = 1,2,...,n to be sample points in the neighborhood of 
0(r)i in n-dimensional space. The idea here is that by training the network to map 
samples in the neighborhood of an exemplar to the exemplar, it will have devel- 
oped a map that can smoothly interpolate (or generalize) to other points around 
the exemplar that may not be in the training set. In this paper we deal with the 
issue of finding the weight matrix that transforms the neural network dynamics into 
such a map. We demonstrate through simulation results that such a map can be 
achieved. For autoassociation, and using error vectors drawn from the training set, 
we show that the method here performs better (in an error-correcting sense) than 
the outer product rule. We are still investigating the performance of the network 
in generalizing to samples outside the training set. 
We construct an n-dimensional neural network system Af to model the underlying 
input-output map according to 
= + 
(1) 
We interpret :r as the neuron activation, g(e(t)) is the neuron output, and W(t) is 
the neural network weight matrix. 
To determine the appropriate W(t), we define an nL-dimensional formal system of 
differential equations, $ 
$: = + = b 
formed by concatenating the equations for A; L times. W. (t) is block-diagonal with 
identical blocks W(t). t9 is the concatenated vector of sample desired outputs,  is 
the concatenated vector of sample inputs. 
The performance index for $ is 
min J = min { 
j=l 
The performance index is chosen to minimize the negative of the correlation between 
the (concatenated) neuron activation and the (concatenated) desired output vectors, 
or equivalently maximize the correlation between the activation and the desired 
output at the final time t f, (the term -a:.'(tl)19). Along the way from initial time 
to to final time tl, the term -z.aP(t)19 under the integral penalizes decorrelation of 
the neuron activation and the desired output. wj(t), j = 1,2,..., n are the rows of 
W(t), and/ is a positive constant. The term/- 5=x wf(t)wj(t)effects a bound 
Neural Network Weight Matrix Synthesis 351 
on the magnitude of the weights. The term 
n L n L 
j=l r=l u=l v=l 
and its meaning will be clear when we examine the optimal path later. 
assumed C  differentiable. 
Proceeding formally[3], we define the HamiltonJan: 
H 
_ 1 _2zT(t)8+Q + Zwf(t)Bwj(t) 
-  
j-1 
g(.) is 
+ x'(t) (-z(t) + w.(Og((O)) 
l(-2z'(t)8+Q+.w(t)Bwj(t))-'(t)z(t) 
=  
j=l 
(4) 
L r 
r=l j=l 
where 
T(t)= [o)(t) (x)2(t ) ... 
is the vector of Lagrange multipliers, and we have used the fact that W, (t) is block- 
diagonal with identical blocks W(t) in writing the summation of the last term in 
the second line of equation (4). The Euler-Lagrange equations are then given by 
= k' / =   - (8 + x(O) +  .(t)x(t) (5) 
= -e (6) 
L 
= � = r + %()((0) (7) 
Owj ?=l 
-x 
0 
From equation (7) we have 
L 
wij(t) = -/E A(r)ig(xJ (r)(t)) (8) 
Choosing 
X(t) = -8 (9) 
satisfies the final condition (6), and with some algebra we find that this choice is 
also consistent with equations (5) and (7). The optimal weight program is therefore 
L 
r=l 
This describes the weight paradigm to be applied to the n-dimensional neural net- 
work.-system A/' in order to model the underlying map described by the sample 
352 Farotimi, Dembo and Kailath 
points. A similar result can be derived for the discrete-time network z(k + 1) = 
W()O(()): 
L 
r..l 
2.1 REMARKS 
Meaning of Q. 
On the optimal path, using equation (10), it is straightforward to show that 
.Q = 
j=l 
Thus Q acts like another integral constraint term on the weights. 
The Optimal Return Function. 
The optimal return function[3], which is the value of the performance index 
on the optimal path can be shown to be 
t) = 
Thus the optimal weight matrix W(t) seeks at every instant to minimize the 
negative correlation (or maximize the correlation) on the optimal path in the 
formal system $ (and hence in the neural network Af). 
� Comparison with outer product rule. 
It is worthwhile to compare equation (10) with the outer product rule: 
L 
w 0 = 3  
r-1 
(11) 
We see that the outer product rule is just one point on the weight trajectory 
defined by equation (10) - the point at final time t! when g(xj(r)(tl)) = 
3 LEARNING CURVES 
In our simulation we considered 14 8-dimensional vectors as the desired outputs. 
The weight synthesis or learning phase is as follows: we initialized the 112-dimensional 
formal synthesis system $ with a corrupted version of the vector set, and used equa- 
tion (10) to find the optimal 8 x 8 weight matrix sequence for an 8-dimensional 
neural network A f to cor
