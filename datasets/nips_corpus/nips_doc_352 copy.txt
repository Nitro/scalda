Stochastic Neurodynamics 
J.D. Cowan 
Department of Mathematics, Committee on 
Neurobiology, and Brain Research Institute, 
The University of Chicago, 5734 S. Univ. Ave., 
Chicago, Illinois 60637 
Abstract 
The main point of this paper is that stochastic neural networks have a 
mathematical structure that corresponds quite closely with that of 
quantum field theory. Neural network Liouvillians and Lagrangians 
can be derived, just as can spin Hamiltonians and Lagrangians in QFT. 
It remains to show the efficacy of such a description. 
1 INTRODUCTION 
A basic problem in the analysis of large-scale neural network activity, is that one can 
never know the initial state of such activity, nor can one safely assume that synaptic 
weights are symmetric, or skew-symmetric. How can one proceed, therefore, to analyse 
such activity? One answer is to use a Master Equation (Van Kampen, 1981). In 
principle this can provide statistical information, moments and correlation functions of 
network activity by making use of ensemble averaging over all possible initial states. In 
what follows I give a short account of such an approach. 
1.1 THE BASIC NEURAL MODEL 
In this approach neurons are represented as simple gating elements which cycle through 
several internal states whenever the net voltage generated at their activated post-synaptic 
62 
Stochastic Neurodynamics 63 
sites exceeds a threshold. These states are quiescent, activated, and refractory, 
labelled 'q', 'a', and 'r' respectively. There are then four transitions to consider: q  a, r  
a, ar, andrq. Two of these, qa, andr a, are functions of the neural 
membrane current. I assume that on the time scale measured in units of m, the 
membrane time constant, the instantaneous transition rate X(q  a) is a smooth function 
of the input current. Ji(T). The transition rates X(q  a) and X(r  a) are then given by: 
and 
Xq = 0[(J(T)/Jq)-11 = 0q[J(T)], 
X r = 0[(J(T)/Jr)-ll = 0r[J(T)], 
(1) 
(2) 
respectively, where Jq and Jr are the threshold currents related to 0 q and 0 r, and where 
0 Ix] is a suitable smoothly increasing function of x, and T = t/ m. . The other two 
transition rates, X (a  r) and X (r  q) are defined simply as constants c and ,. Figure 1 
shows the kinetic scheme that results. Implicit in this scheme is the smoothing of input 
current pulses that takes place in the membrane,and also the smoothing caused by the 
81 oc 
8 2 
q r' 
Figure 1. Neural state transition rates 
presumed asynchronous activation of synapses. This simplified description of neural 
state transitions is essential to our investigation of cooperative effects in large nets. 
1.2 PROBABILITY DISTRIBUTIONS FOR NEURAL NETWORK ACTIVITY 
The configuration space of a neural network is the space of distinguishable patterns of 
neural activity. Since each neuron can be in the state q, a or r, there are 3 N such patrems 
in a network of N neurons. Since N is 0(1010), the configuration space is in principle 
very large. This observation, together with the existence of random fluctuations of neural 
64 Cowan 
activity, and the impracticability of specifying the initial states of all the neurons in a 
large network, indicates the need for a probabilistic description of the formation and de- 
cay of patterns of neural activity. 
Let Q(T), A(T), R(T) denote the numbers of quiescent, activated, and refractory neurons 
in a network of N neurons at time T. Evidently, 
Q(T)+A(T)+R(T) = N, 
(3) 
Consider therefore N neurons in a d-dimensional lattice. 
denoted by 
I .Q > = IVlV2 ......... VN> 
Let a neural state vector be 
(4) 
where vi means the neuron at the site i is in the state v = q, a, or r. Let P[l'l (T)] be the 
probability of finding the network in state I /> at time T, and let 
I P(T) > = Z P[(T)]I > (5) 
be a neural probability state vector. Evidently ZP[I'I(T)] = 1. (6) 
1.3 A NEURAL NETWORK MASTER EQUATION 
Now consider the most probable state transitions which can occur in an asynchronous 
noisy network. These are: 
(Q, A, R)- (Q, A, R) 
(Q+I, A-l, R)-' (Q, A, R) 
(Q, A-l, R+0-' (Q, A, R) 
(Q, A+L R-D - (Q, A, R) 
(Q-, A, R+0 - (Q, A, R) 
All other transitions, e.g., those involving two 
assumed to occur with probability O(dT). 
no change 
activation of a quiescent cell 
activation of a refractory cell 
an activated cell becomes refractory 
a refractory cell beomes quiescent. 
or more transitions in time dT, are 
These state transitions can be represented by the action on a set of basis vectors, of 
certain matrices. Let the basis vectors be: 
Stochastic Neurodynamics 65 
and consider the Gell-Mann matrices representing the Lie Group SU(3) (Georgi, 1982): 
and the raising and lowering operators: 
1 1 
A_+I = (X4 + i)5), A+2 = (3,1+ i)2), 
(8) 
1 
A + 3 =  (  i 3.7). (9) 
It is easy to see that these operators act on the basis vectors I v > as shown in figure 2. 
A+I 
A 
q 
A_ 3 
Figure 2. Neural State Transitions generated by the 
raising and lowering operators of the Lie Group 
SU(3). 
It also follows that: 
A= A+liA_li = ZA+2iA_2i (10) 
i i 
and that: Ji =  wijA+ljA.lj=  wijA+2jA-2j. (11) 
J J 
The entire sequence of neural state transition into (Q,A,R) can be represented by the 
operator Liouvillian: 
66 Cowan 
L=t�.(A+2i- 1) A_2i + [�.(A+3i- 1) A_3i 
i  
1 1 
+  . (A -li - 1) A +li 0q[Ji] +  . (A -2i - 1) A +2i 0r[Ji] � 
1 1 
This operator acts on the state function I P(T)> according to the equation: 
(12) 
OT I P(T)> =- L I P(T)>. (13) 
This is the neural network analogue of the Schr6dinger equation, except that P[l'/(T)] = 
< l'/IP(T)> is a real probability distribution, and L is not Hermitian. In fact this equation 
is a Markovian representation of neural network activity (Doi, 1976; Grassberger & 
Scheunert, 1980), and is the required master equation. 
1.4 A SPECIAL CASE: TWO-STATE NEURONS 
It is helpful to consider the simpler case of two state neurons first, since the group 
algebra is much simpler. I therefore neglect the refractory state, and use the two 
dimensional basis vectors: 
Iq>=(i ) , la>=(l ) (14) 
corresponding to the kinetic scheme shown in figure 3a: 
0! 
(a) (b) 
Figure 3. (a) Neural State Transitions in the two- 
state case, (b) Neural State Transitions generated by 
the raising and lowering operators of the Lie Group 
su(2). 
Stochastic Neurodynamics 67 
The relevant matrices are the well-known Pauli spin matrices representing the Lie Group 
SU(2) (Georgi, 1982): 
and the raising and lowering operators: 
1 
c+ =  (Cl + ic2) 
giving the state transiiton diagram shown in figure 3(b). 
Liouvillian is: 
cB= (1 _'1) (15) 
(16) 
The corresponding neural 
1 
L = at(c+i- 1) eLi + Z (c-I - 1) c+i0q[Ji] (17) 
i 1 
where 
Physicists will recognize 
Hamiltonian of QFT: 
Ji =  wij c+j c_j. (18) 
J 
this Liouvillian as a generalization of the Regge spin 
L = atZ(c+i- 1) eLi + Z .(CL1 1) c+ic+jG_j. (19) 
i  j 
In principle, eqn. (13) with L given by eqn. (12) or (17), together with initial conditions, 
contains a complete description of neural network activity, since its formal solution takes 
the form: 
T 
I P(T)> = exp {-  L(T')dT'}I P(0)>. (20) 
0 
1.5 MOMENT GENERATING EQUATIONS AND SPIN-COHERENT STATES 
Solving this system of equation in detail however, is a difficult problem. In practice one 
is satisfied with the first few statistical moments. These can be obtained as follows (I 
describe here the two-state case. Similar but more complicated calculations obtain for 
the three-state case). 
Consider the following spin-coherent states (Perelomov 1986; Hecht 1987): 
<atl =<01exp( atic_i), lat> = exp( oc+i) 10> 
i i 
where at is a complex number, and < 0 1 is the vacuum state < qlq2 ...... qN I. 
Evidently 
(21) 
68 Cowan 
<a[P>= 
It can be shown that 
< a l : < a 
 VN and that 
< a I> = ! a 2 ............. a N 
<alP> = 
G( a 1 a 2 .... CXl) the moment generating function for the probability distribution P(T). 
It can then be shown that: 
3G +  . (--- 1) Dai0q[Jil ] G (22) 
3T-[aY(Do-1) 3 1 3 
i 3a i i 3a i 
where D = (1- a i ) and Ji =  wij Doj  (23) 
3a i j 3aj 
i.e.; the moment generating equation expressed in the oscillator-algebra representation. 
1.5 A NEURAL NETWORK PATH INTEGRAL 
The content of eqns. (22) and (23) can be summarized in a Wiener-Feynman Path 
Integral (Schulman 1981). It can be shown that the transition probability of reaching a 
state l'l' (T) given the initial state l'l(T0), the so-called propagator ,(l'l', T I l'l, TO), 
can be expressed as the Path Integral: 
TO 
rlI)ai(T')exp [  {  (D'aiDa i -Da i D'a i )- L(Dai,Da i ) }], (24) 
i T i 
where D'a i- -TDai andI) a i(T') = (2)nlim 
n d2aiO) 
I1- >oo FI : 
j=l (l+ai(j)a i (j))3 
, and 
where d2a = d(R1 a) d(Im a). This propagator is sometimes written as an expectation 
with respect to the Wiener measure  .rI ida i (T') as: 
1 
TO 
G(n'ln) = <exp[- I dT' L]> 
T 
where the neural network Lagrangian is defined as: 
(25) 
Stochastic Neurodynamics 69 
* 1 * * 
f.., = L(Dcti,Doi) - .  (D'ctiDcz i -Dot i D'ot i ). (26) 
1 
The propagator , contains all the statistics of the network activity. Steepest descent 
methods, asymptotics, and Liapunov-Schmidt bifurcation methods may be used to 
evaluate it. 
2 CONCLUSIONS 
The main point of this paper is that stochastic neural networks have a mathematical 
structure that corresponds quite closely with that of quantum field theory. Neural 
network Liouvillians and Lagrangians can be derived, just as can spin Hamiltonians and 
Lagrangians in QFT. It remains to show the efficacy of such a description. 
Acknowledgements 
The early stages of this work were carded out in part with Alan Lapedes and David Sharp 
of the Los Alamos National Laboratory. We thank the Santa F6 Institute for hospitality 
and facilities during this work, which was supported in part by grant # N00014-89-J-1099 
from the US Department of the Navy, Office of Naval Research. 
References 
Van Kampen
