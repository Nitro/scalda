Optimal signalling in Attractor Neural 
Networks 
Isaac Meilijson Eytan Ruppin * 
School of Mathematical Sciences 
Raymond and Beverly Sackler Faculty of Exact Sciences 
Tel-Aviv University, 69978 Tel-Aviv, Israel. 
Abstract 
In [Meilijson and Ruppin, 1993] we presented a methodological 
framework describing the two-iteration performance of Hopfield- 
like attractor neural networks with history-dependent, Bayesian 
dynamics. We now extend this analysis in a number of directions: 
input patterns applied to small subsets of neurons, general con- 
nectivity architectures and more efficient use of history. We show 
that the optimal signal (activation) function has a slanted sigmoidal 
shape, and provide an intuitive account of activation functions with 
a non-monotone shape. This function endows the model with some 
properties characteristic of cortical neurons' firing. 
I Introduction 
It is well known that a given cortical neuron can respond with a different firing pat- 
tern for the same synaptic input, depending on its firing history and on the effects 
of modulator transmitters (see [Connors and Gutnick, 1990] for a review). The time 
span of different channel conductances is very broad, and the influence of some ionic 
currents varies with the history of the membrane potential [Lytton, 1991]. Moti- 
vated by the history-dependent nature of neuronal firing, we continue.our previous 
investigation [Meilijson and Ruppin, 1993] (henceforth, M & R) describing the per- 
formance of Hopfield-like attractor neural networks (ANN) [Hopfield, 1982] with 
history-dependent dynamics. 
*Currently in the Dept. of Computer science, University of Maryland 
485 
486 Meilijson and Ruppin 
Building upon the findings presented in M & R, we now study a more general 
framework: 
We differentiate between 'input' neurons receiving the initial input signal 
with high fidelity and 'background' neurons that receive it with low fidelity. 
Dynamics now depend on the neuron's history of firing, in addition to its 
history of input fields. 
The dependence of ANN performance on the network architecture can be 
explicitly expressed. In particular, this enables the investigation of cortical- 
like architectures, where neurons are randomly connected to other neurons, 
with higher probability of connections formed between spatially proximal 
neurons [Braitenberg and Schuz, 1991]. 
Our goal is twofold: first, to search for the computationally most efficient history- 
dependent neuronal signal (firing) function, and study its performance with relation 
to memoryless dynamics. As we shall show, optimal history-dependent dynamics 
are indeed much more efficient than memoryless ones. Second, to examine the 
optimal signal function from a biological perspective. As will shall see, it shares 
some basic properties with the firing of cortical neurons. 
2 The model 
Our framework is an ANN storing m + 1 memory patterns ,2,...,m+x, each 
an N-dimensional vector. The network is composed of N neurons, each of which 
is randomly connected to K other neurons. The (m + 1)N memory entries are 
independent with equally likely :t:l values. The initial pattern X, signalled by 
L(_< N) initially active neurons, is a vector of :t:l's, randomly generated from one 
of the memory patterns (say  = m+l) such that P(Xi = i) = 1+� for each of 
2 
1+, 
the L initially active neurons and P(Xi = i) = 2 for each initially quiescent 
(non-active) neuron. Although e,/  [0, 1) are arbitrary, it is useful to think of e 
as being 0.5 (corresponding to an initial similarity of 75%) and of/ as being zero 
- a quiescent neuron has no prior preference for any given sign. Let ax = rn/nx 
denote the initial memory load, where nx = LK/N is the average number of signals 
received by each neuron. 
The notion of 'iteration' is viewed as an abstraction of the overall dynamics for 
some length of time, during which some continuous input/output signal function 
(such as the conventional sigmoidal function) governs the firing rate of the neuron. 
We follow a Bayesian approach under which the neuron's signalling and activation 
decisions are based on the a-posteriori probabilities assigned to its two possible true 
memory states, :t:l. 
Initially, neuron i is assigned a prior probability hi � - P( = llXi Ii ()) = 14-e 
-- ' 2 
or 1___ which is conveniently expressed as hi � - x where, letting g(t) - 
-- lq.e_2gi(O ) ' 
- log l+t 
2 1-t' 
#i(o) _ ( #(e)Xi if i is active 
#()Xi if i is silent 
Optimal Signalling in Attractor Neural Networks 487 
The input field observed by neuron i as a result of the initial activity is 
N 
nl j=l 
where Ij () = 0, 1 indicates whether neuron j has fired in the first iteration, Iij = 0, 1 
indicates whether a connection exists from neuron j to neuron i and Wij denotes 
its magnitude, given by the Hopfield prescription 
rn+l 
(2) 
As a result of observing the input field fi(1), which is approximately normally 
distributed (given i,Xi and Ii()), neuron i changes its opinion about {i - 1} 
from hi � to 
( ) 1 
Ai (1) = P i = llXi,//(1) fi(1) = (3) 
' 1 q- e -201() ' 
expressed in terms of the (additive) generalized field gi(D = gi � + fi (). 
We now get to the second iteration, in which, as in the first iteration, some of the 
neurons become active and signal to the network. We model the signal function 
neuron i emits as h(gi(), Xi, Ii()). The field observed by neuron i (with n2 updating 
neurons per neuron) is 
N 
i,(2) = 1 
n2 j=l 
(4) 
on the basis of which neuron i computes its posterior belief A, � = P(i = 
11Xi , Ii (), fi (), fi �) and expresses its final choice of sign as Xi (2) = sign(Ai � - 
0.5). The two-iteration performance of the network is measured by the final simi- 
larity 
S! = 1 + ! = P(Xi(2 ) = i) = 1 +  E7=1Xj(2)j (5) 
2 2 
3 Analytical results 
The goals of our analysis have been: A. To present an expression for the performance 
under arbitrary architecture and activity parameters, for general signal functions 
h0 and h. B. Use this expression to find the best choice of signal functions which 
maximize performance. We show the following: 
The neuron's final decision is given by 
Xi � = Sign [(Ao + Boli(D)Xi + Alfi (D + A2fi (2)] (6) 
for some constants A0, B0, A and A2. 
488 Meilijson and Ruppin 
The performance achieved is 
where, for some Aa > 0 
n* nl q- mA3 ' 
2 + 2  x x 
d  is the standard normal cumulative distribution function. 
(7) 
(8) 
(9) 
The optimal analog signal function, illustrated in figure 1, is 
ho = h(gi (x), q,1, 0)= 
h = h(gi(X),+l,1)= R(gi(1),e)- 1 
where, for some A4 > 0 and A5 > 0, R(s,t) = A4 tanh(s) - As(s - g(t)). 
(10) 
(a) (b) 
4.0 
-- Silent neurons 
.... Acve neurons 
...... 
2.0 
V 
0.0 
-V 
..0 
Signal 
,5 i Input field 
-4.0 
-5.0 -3.0 -1.0 1.0 3.0 5.0 
Input field 
Figure 1: (a) A typical plot of the slanted sigmoid, Network parameters are N - 
5000, K - 3000, n = 200 and m = 50. (b) A sketch of its discretized version. 
The nonmonotone form of these functions, illustrated in figure 1, is clear. Neurons 
that have already signalled +1 in the first iteration have a lesser tendency to send 
positive signals than quiescent neurons. The signalling of quiescent neurons which 
receive no prior information (5 = 0) has a symmetric form. The optimal signal is 
shown to be essentially equal to the sigmoid modified by a correction term depending 
only on the current input field. In the limit of low memory load (e/vfG' --. oo), the 
best signal is simply a sigmoidal function of the generalized input field. 
Optimal Signalling in Attractor Neural Networks 489 
To obtain a discretized version of the slanted sigmoid, we let the signal be sign(h(y)) 
as long as Ih(y)l is big enough - where h is the slanted sigmoid. The resulting signal, 
as a function of the generalized field, is (see figure la and lb) 
+1 y < fix (j) or/?4 (i) < y < fi5 (j) 
hj(y) = -1 y > fi6 (i) or fi2 (j) < y < fia (j) (11) 
otherwise 
where -oo < fil � < 2 � <_ fi3 � < 4 � 5 5 � < 6 � <  and - < 1 (1) < 
2(1) g a()) < 4(1)  (1) < () <  define, respectively, the firing pattern of 
the neurons that were silent or active in the first iteration. To find the best such 
discretized version of the optimal signal, we search numerically for the activity level 
v which mimizes performance. Every activity level v, used as a threshold on Ih(u)l, 
defines the (at most) twelve parameters O) (which are identified numerically via 
the Newton-aphson method)  illustrated in figure lb. 
4 Numerical Results 
(a) (b) 
Ilty-baed tgnalling 
] ..... Discretlzed ;tnalllng 
..... Analog optimal tgnalllng 
-- Discrete signalling 
......... Analog signalling 
0.,5 , A , I ,  .  , 0.940 , . I , t , I . F , 
0.0 1000.0 2000.0 30(10.0 4000.0 5000.0 0.0 1000.0 2000.0 3000.0 4000.0 5000.0 
K K 
Figure 2: Two-iteration performance as a function of connectivity K. (a) Network 
parameters are N = 5000, n = 200, and m = 50. All neurons receive their input 
state with similar initial overlap e =/ = 0.5. (b) Network parameters are N = 5000, 
m=50, nx=200, e=0.5andS=0. 
Using the formulation presented in the previous section, we investigated numeri- 
cally the two-iteration performance achieved in several network architectures with 
optimal analog signalling and its discretization. Already in small scale networks of a 
few hundred neurons our theoretical calculations correspond fairly accurately with 
490 Meilijson and Ruppin 
0.970 
simulation results. First we repeat the example of a corticM-like network investi- 
gated in M & R, but now with optimal analog and discretized signalling. The nearly 
identical marked superiority of optimal analog and discretized dynamics over the 
previous, posterior-probability-based signalling is evident, as shown in figure 2 (a). 
While low activity is enforced in the first iteration, the number of neurons allowed 
to become active in the second iteration is not restricted, and best per
