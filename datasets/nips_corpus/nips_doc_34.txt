290 
CYCLES: A Simulation Tool for Studying 
Cyclic Neural Networks 
Michael T. Gately 
Texas Instruments Incorporated, Dallas, TX 75265 
ABSTRACT 
A computer program has been designed and implemented to allow a researcher 
to analyze the oscillatory behavior of simulated neural networks with cyclic con- 
nectivity. The computer program, implemented on the Texas Instruments Ex- 
plorer/Odyssey system, and the results of numerous experiments are discussed. 
The program, CYCLES, allows a user to construct, operate, and inspect neural 
networks containing cyclic connection paths with the aid of a powerful graphics- 
based interface. Numerous cycles have been studied, including cycles with one or 
more activation points, non-interruptible cycles, cycles with variable path lengths, 
and interacting cycles. The final class, interacting cycles, is important due to its 
ability to implement time-dependent goal processing in neural networks. 
INTRODUCTION 
Neural networks are capable of many types of computation. However, the 
majority of researchers are currently limiting their studies to various forms of 
mapping systems; such as content addressable memories, expert system engines, 
and artificial retinas. Typically, these systems have one layer of fully connected 
neurons or several layers of neurons with limited (forward direction only) connec- 
tivity. I have defined a new neural network topology; a two-dimensional lattice of 
neurons connected in such a way that circular paths are possible. 
The neural networks defined can be viewed as a grid of neurons with one 
edge containing input neurons and the opposite edge containing output neurons 
[Figure 1]. Within the grid, any neuron can be connected to any other. Thus 
from one point of view, this is a multi-layered system with full connectivity. I 
view the weights of the connections as being the long term memory (LTM) of the 
system and the propagation of information through the grid as being it's short 
term memory (STM). 
The topology of connectivity between neurons can take on any number of 
patterns. Using the mammalian brain as a guide, I have limited the amount of 
connectivity to something much less then total. In addition to' making analysis 
of such systems less complex, limiting the connectivity to some small percentage 
of the total number of neurons reduces the amount of memory used in computer 
simulations. In general, the connectivity can be purely random, or can form any 
of a number of patterns that are repeated across the grid of neurons. 
The program CYCLES allows the user to quickly describe the shape of the 
neural network grid, the source of input data, the destination of the output data, 
the pattern of connectivity. Once constructed, the network can be run. during 
which time the STM may be viewed graphically. 
@ American Institute of Physics 1988 
291 
 0 O00000 O00000  
o 00 O0000 O00000 o 
00 O0000000 O00 
00 O00000 
O0 O0 O000 
00000000 
0 O000000 
00 O00000 
00000 O00 
8'-' 0 0 0 0 0 0 0 0 0 0 0 0 0 
292 
IMPLEMENTATION 
CYCLES was implemented on a TI Explorer/Odyssey computer system with 
8MB of RAM and 128MB of Virtual Memory. The program was written in Com- 
mon LISP. The program was started in July of 1986, put aside for a while, and 
finished in March of 1987. Since that time, numerous small enhancements have 
been made - and the system has been used to test various theories of cyclic neural 
networks. 
The code was integrated into the Neural Network Workstation (NNW), an 
interface to various neural network algorithms. The NNW utilizes the window 
interface of the Explorer LISP machine to present a consistent command input 
and graphical output to a variety of neural network algorithms [Figure 2]. 
The backpropagation-like neurons are collected together into a large three- 
dimensional array. The implementation actually allows the use of multiple two- 
dimensional grids; to date, however, I have studied only single-grid systems. 
Each neuron in a CYCLES simulation consists of a list of information; the 
value of the neuron, the time that the neuron last fired, a temporary value used 
during the computation of the new value, and a list of the neurons connectivity. 
The connectivity list stores the location of a related neuron and the strength of 
the connection between the two neurons. Because the system is implemented in 
arrays and lists, large systems tend to be very slow. However, most of my analysis 
has taken place on very small systems (< 80 neurons) and for this size the speed 
is acceptable. 
To help gauge the speed of CYCLES, a single grid system containing 100 
neurons takes 0.8 seconds and 1235 cons cells (memory cells) to complete one 
update within the LISP machine. If the graphics interface is disabled, a test 
requiring 100 updates takes a total of 10.56 seconds. 
TYPES OF CYCLES 
can 
1. 
o 
As mentioned above, several types of cycles have been observed. Each of these 
be used for different applications. Figure 3 shows some of these cycles. 
SIMPLE cycles are those that have one or more points of activation traveling 
across a set number of neurons in a particular order. The path length can be 
any size. 
NON-INTERRUPTABLE cycles are those that have sufficiently strong con- 
nectivity strengths that random flows of activation which interact with the 
cycle will not upset or vary the original cycle. 
VARIABLE PATH LENGTH cycles can, based upon external information, 
change their path length. There must be one or more neurons that are always 
a part of the path. 
INTERACTING cycles typically have one neuron in common. Each cycle 
must have at least one other neuron involved at the junction point in order to 
keep the cycles separate. This type of cycle has been shown to implement a 
complex form of a clock where the product of the two (or more) path lengths 
are the fundamental frequency. 
293 
Figure 3. Types of Cycles [Simple and Interacting] 
� � � � � 
� � � 
� � � 
Figure 4. Types of Connectivity [Nearest Neighbor and Gaussian] 
INPUT OUTPUT 
Intent 
Joint 3 Extended 
Joint 2 Centered 
Joint I Extended 
Chuck Opened 
Chuck Closed 
) ) Completed 
0 0 Move Joint3 
0  Move Joint2 
0 Move Joint 1 
0 0 0 Open Chuck 
0 Close Chuck 
Figure 5. Robot Arm used in Example 
294 
CONNECTIVITY 
Several types of connectivity have been investigated. These are shown in 
Figure 4. 
1. In TOTAL connectivity, every neuron is connected to every other neuron. 
This particular pattern produces very complex interactions with no apparent 
stability. 
2. With RANDOM connectivity, each neuron is connected to a random number 
of other neurons. These other neurons can be anywhere in the grid. 
3. A very useful type of connectivity is to have a PATTERN. The patterns can 
be of any shape, typically having one neuron feed its nearest neighbors. 
4. Finally, the GAUSSIAN pattern has been used with the most success. In this 
pattern, each neuron is connected to a set number of nodes - but the selection 
is random. Further, the distribution of nodes is in a Gaussian shape, centered 
around a point forward of itself. Thus the flow of information, in general, 
moves forward, but the connectivity allows cycles to be formed. 
ALGORITHM 
The algorithm currently being used in the system is a standard inner product 
equation with a sigmoidal threshold function. Each time a neuron's weight is to 
be calculated, the value of each contributing neuron on the connectivity list is 
multiplied by the strength of the connection and summed. This sum is passed 
through a sigmoidal thresholding function. The value of the neuron is changed 
to be the result of this threshold function. As you can see, the system updates 
neurons in an ordered fashion, thus certain interactions will not be observed. Since 
timing information is saved in the neurons, asynchrony could be simulated. 
Initially, the weights of the connections are set randomly. A number of inter- 
esting cycles have been observed as a result of this randomness. However, several 
experiments have required specific weights. To accommodate this, an interface to 
the weight matrix is used. The user can create any set of connection strengths 
desired. 
I have experimented with several learning algorithms-that is, algorithms that 
change the connection weights. The first mechanism was a simple Hebbian rule 
that states that if two neurons both fire, and there is a connection between them, 
then strengthen the strength of that connection. A second algorithm I experi- 
mented with used a pain/pleasure indicator to strengthen or weaken weights. 
An algorithm that is currently under development actually presets the weights 
from a grammar of activity required of the network. Thus, the user can describe 
a process that must be controlled by a network using a simple grammar. This 
description is then compiled into a set of weights that contain cycles to indicate 
time-independent components of the activity. 
295 
USAGE 
Even without a biological background, it is easy to see that the processing 
power of the human brain is far more than present associative memories. Our 
repertoire of capabilities includes, among other things: memory of a time line, 
creativity, numerous types of biological clocks, and the ability to create and ex- 
ecute complex plans. The CYCLES algorithm has been shown to be capable of 
executing complex, time-variable plans. 
A plan can be defined as a sequence of actions that must be performed in 
some preset order. Under this definition, the execution of a plan would be very 
straightforward. However, when individual actions within the plan take an inde- 
terminate length of time, it is necessary to construct an execution engine capable 
of dealing with unexpected time delays. Such a system must also be able to abort 
the processing of a plan based on new data. 
With careful programming of connection weights, I have been able to use 
CYCLES to execute time-variable plans. The pa
