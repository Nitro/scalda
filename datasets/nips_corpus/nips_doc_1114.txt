Symplectic Nonlinear Component 
Analysis 
Lucas C. Parra 
Siemens Corporate Research 
755 College Road East, Princeton, NJ 08540 
lucas@scr.siemens.com 
Abstract 
Statistically independent features can be extracted by finding a fac- 
torial representation of a signal distribution. Principal Component 
Analysis (PCA) accomplishes this for linear correlated and Gaus- 
sian distributed signals. Independent Component Analysis (ICA), 
formalized by Comon (1994), extracts features in the case of lin- 
ear statistical dependent but not necessarily Gaussian distributed 
signals. Nonlinear Component Analysis finally should find a facto- 
rial representation for nonlinear statistical dependent distributed 
signals. This paper proposes for this task a novel feed-forward, 
information conserving, nonlinear map - the explicit symplectic 
transformations. It also solves the problem of non-Gaussian output 
distributions by considering single coordinate higher order statis- 
tics. 
I Introduction 
In previous papers Deco and Brauer (1994) and Parra, Deco, and Miesbach (1995) 
suggest volume conserving transformations and factorization as the key elements 
for a nonlinear version of Independent Component Analysis. As a general class 
of volume conserving transformations Parra et al. (1995) propose the symplectic 
transformation. It was defined by an implicit nonlinear equation, which leads to a 
complex relaxation procedure for the function recall. In this paper an explicit form 
of the symplectic map is proposed, overcoming thus the computational problems. 
438 L. C. PARRA 
In order to correctly measure the factorization criterion for non-Gaussian output 
distributions, higher order statistics has to be considered. Comon (1994) includes 
in the linear case higher order cumulants of the output distribution. Deco and 
Brauer (1994) consider multi-variate, higher order moments and use them in the 
case of nonlinear volume conserving transformations. But the calculation of multi- 
coordinate higher moments is computational expensive. 
The factorization criterion for statistical independence can be expressed in terms of 
minimal mutual information. Considering only volume conserving transformations 
allows to concentrate on single coordinate statistics, which leads to an important 
reduction of computational complexity. So far, this approach (Deco & Schiirman, 
1994; Parra et al., 1995) has been restricted to second order statistic. The present 
paper discusses the use of higher order cumulants for the estimation of the single 
coordinate output distributions. The single coordinate entropies measured by the 
proposed technique match the entropies of the sampled data more accurately. This 
leads in turns to better factorization results. 
2 Statistical Independence 
More general than decorrelation used in PCA the goal is to extract statistical 
independent features from a signal distribution p(x). We look for a determinis- 
tic transformation on n: y = f(x) which generates a factoffal representation 
P(Y) = rli p(Yi), or at least a representation where the individual coordinates p(Yi) 
of the output variable y are as factoffal as possible. This can be accomplished 
by minimizing the mutual information MI[p(y)]. 
0 _ MI[p(y)] = E H[p(yi)]- Hip(y)], (1) 
i----1 
since MI[p(y)] - 0 holds if p(y) is factorial. The mutual information can be used 
as a measure of independence. The entropies H in the definition (1) are defined 
as usual by Hip(y)] - - f_ p(y) in p(y) dy. 
As in linear PCA we select volume conserving transformations, but now without 
restricting ourselves to linearity. In the noise-free case of reversible transformations 
volume conservation implies conservation of entropy from the input x to the output 
y, i.e. H[p(y)] = H[p(x)] = const (see Papoulis, 1991). The minimizationof mutual 
information (1) reduces then to the minimization of the single coordinate output 
entropies H[p(yi)]. This substantially simplifies the complexity of the problem, 
since no multi-coordinate statistics is required. 
2.1 Measuring the Entropy with Cumulants 
With an upper bound minimization criterion the task of measuring entropies can 
be avoided (Parra et al., 1995): 
1 
lln(2e) + lna. 
H[p(yi)] _   
(2) 
Symplectic Nonlinear Component Analysis 439 
0.8 
0'7 t 
0.6 
0.4 
0.3 
0.2 
Edgeworth approxmatn to second and forth order 
0.1 
0 
'0.1.4 
Yl 
dQ(y)/dy 
Xl X 2 
1 
Figure 1: LEFT: Doted line: exponential distribution with additive Gaussian noise 
sampled with 1000 data points. (noise-variance/decay-constant = 0.2). Dashed 
line: Gaussian approximation equivalent to the Edgeworth approximation to second 
order. Solid line: Edgeworth approximation including terms up to fourth order. 
RIGHT: Structure of the volume conserving explicit symplectic map. 
The minimization of the individual output coordinate entropies H[p(yi)] simplifies 
to the minimization of output variances (ri. For the validity of that approach it is 
crucial that the map y = f(x) transforms the arbitrary input distribution p(x) into 
a Gaussian output distribution. But volume conserving and continuous maps can 
not transform arbitrary distributions into Gaussians. To overcome this problem one 
includes statistics - higher than second order - to the optimization criterion. 
Comon (1994) suggests to use the Edgeworth expansion of a probability distribu- 
tion. This leads to an analytic expression of the entropy in terms of measurable 
higher order cumulants. Edgeworth expands the multiplicative correction to the 
best Gaussian approximation of the distribution in the orthonormal basis of Her- 
mite polynomials ha(y). The expansion coefficients are basically given by the cu- 
mulants ca of distribution p(y). The Edgeworth expansions reads for a zero-mean 
distribution with variance (r 2, (see Kendall & Stuart, 1969) 
p(y) 
f(y) 
(3) 
Note, that by truncating this expansion at a certain order, we obtain an approx- 
imation Papp(Y), which is not strictly positive. Figure 1, left shows a sampled 
exponential distribution with additive Gaussian noise. 
By cutting expansion (3) at fourth order, and further expanding the logarithm in 
definition of entropy up to sixth order, Comon (1994) approximates the entropy by, 
440 L. C. PARRA 
i c i c4 2 7 c a lc c4 
1 ln(2re) + In o- -- (4) 
H[p(y)app] ,  12 r 6 48 r 8 48 r u + 8 r 6 r 4 
We suggest to use this expression to minimize the single coordinate entropies in the 
definition of the mutual information (1). 
2.2 Measuring the Entropy by Estimating an Approximation 
Note that (4) could only be obtained by truncating the expansion (3). It is there- 
fore limited to fourth order statistic, which might be not enough for a satisfactory 
approximation. Besides, the additional approximation of the logarithm is accurate 
only for small corrections to the best Gaussian approximation, i.e. for f(y)  1. 
For distributions with non-Gaussian tails the correction terms might be rather large 
and even negative as noted above. We therefore suggest alternatively, to measure 
the entropy by estimating the logarithm of the approximated distribution in Papp (Y) 
with the given data points y and using Edgeworth approximation (3) for Papp (Y), 
i N 
--- E In Papp (Y) - const + In (r - -- E in f(y) 
Hip(y)] , N u_-i N ,--1 
(5) 
Furthermore, we suggest to correct the truncated expansion Papp by setting 
lapp (Y) -+ 0 for all lapp (Y) < 0. For the entropy measurement (5) there is in 
principle no limitation to any specific order. 
In table 1 the different measures of entropy are compared. The values in the row 
labeled 'partition' are measured by counting the numbers n(i) of data points falling 
in equidistant intervals i of width Ay and summing-p(i)Ay lnp(i) over all intervals, 
with p(i)Ay = n(i)/N. This gives good results compared to the theoretical values 
only because of the relatively large sampling size. These values are presented here 
in order to have an reliable estimate for the case of the exponential distribution, 
where cumulant methods tend to fail. 
The results for the exponential distribution show the difficulty of the measurement 
proposed by Comon, whereas the estimation measurement given by equation (5) is 
stable even when considering (for this case) unreliable 5th and 6th order cumulants. 
The results for the symmetric-triangular and uniform distribution demonstrate the 
insensibility of the Gaussian upper bound for the example of figure 2. A uniform 
squared distribution is rotated by an angle c. On the abscissa and ordinate a 
triangular or uniform distribution are observed for the different angles c: II/4 
or c = 0 respectively. The approximation of the single coordinate entropies with 
a Gaussian measure is in both cases the same. Whereas measurements including 
higher order statistics correctly detect minimal entropy (by fixed total information) 
for the uniform distribution at c = 0. 
3 Explicit Symplectic Transformation 
Different ways of realizing a volume conserving transformation that guarantees 
H[p(x)] = H[p(x)] have been proposed (Deco & Schiirman, 1994; Parra et al., 
Symplectic Nonlinear Component Analysis 441 
Measured entropy of Gauss uniform triangular exponential 
sampled distributions symmetric + Gauss noise 
partition 1.35 q- .02 .024 q- .006 .14 q- .02 1.31 q- .03 
Gaussian upper bound (2) 1.415 q- .02 .18 q- .016 .18 q- .02 1.53 q- .04 
Comon, eq. (4) 1.414 q- .02 .14 q- .015 .17 q- .02 3.0 q- 2.5 
Estimate (5) - 4th order 1.414 q- .02 .13 q- .015 .17 q- .02 1.39 q- .05 
Estimate (5) - 6th order 1.414 q- .02 .092 q- .001 .16 q- .02 1.3 q- .5 
theoretical value 1.419 .0 .153 
Table 1: Entropy values for different distributions sampled with N -- 1000 data 
points and the different estimation methods explained in the text. The standard 
deviations are obtained by multiple repetition of the experiment. 
1995). A general class of volume conserving transformations are the symplectic 
ma
