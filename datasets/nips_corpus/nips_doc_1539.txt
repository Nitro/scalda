Blind Separation of Filtered Sources 
Using State-Space Approach 
Liqing Zhang* and Andrzej Cichocki t 
Laboratory for Open Information Systems, 
Brain Science Institute, RIKEN 
Saitama 351-0198, Wako shi, JAPAN 
Email: zha, cia)@open.brain.riken.go.jp 
Abstract 
In this paper we present a novel approach to multichannel blind 
separation/generalized deconvolution, assuming that both mixing 
and demixing models are described by stable linear state-space sys- 
tems. We decompose the blind separation problem into two pro- 
cess: separation and state estimation. Based on the minimization 
of Kullback-Leibler Divergence, we develop a novel learning algo- 
rithm to train the matrices in the output equation. To estimate the 
state of the demixing model, we introduce a new concept, called 
hidden innovation, to numerically implement the Kalman filter. 
Computer simulations are given to show the validity and high ef- 
fectiveness of the state-space approach. 
I Introduction 
The field of blind separation and deconvolution has grown dramatically during re- 
cent years due to its similarity to the separation feature in human brain, as well as its 
rapidly growing applications in various fields, such as telecommunication systems, 
image enhancement and biomedical signal processing. The blind source separation 
problem is to recover independent sources from sensor outputs without assuming 
any priori knowledge of the original signals besides certain statistic features. Refer 
to review papers [1] and [5] for the current state of theory and methods in the field. 
Although there exist a number of models and methods, such as the infomax, nat- 
ural gradient approach and equivariant adaptive algorithms, for separating blindly 
independent sources, there still are several challenges in generalizing mixture to dy- 
* On leave from South China University of Technology, China 
t On leave from Warsaw University of Technology, Poland 
Blind Separation of Filtered Sources 649 
namic and nonlinear systems, as well as in developing more rigorous and effective 
algorithms with general convergence.[1-9], [11-13] 
The state-space description of systems is a new model for blind separation and 
deconvolution[9,12]. There are several reasons why we use linear state-space systems 
as blind deconvolution models. Although transfer function models are equivalent 
to the state-space ones, it is difficult to exploit any common features that may 
be present in the real dynamic systems. The main advantage of the state space 
description for blind deconvolution is that it not only gives the internal description 
of a system, but there are various equivalent types of state-space realizations for a 
system, such as balanced realization and observable canonical forms. In particular 
it is known how to parameterize some specific classes of models which are of interest 
in applications. Also it is much easy to tackle the stability problem of state-space 
systems using the Kalman Filter. Moreover, the state-space model enables much 
more general description than standard finite impulse response (FIR) convolutive 
filtering. All known filtering (dynamic) models, like AR, MA, ARMA, ARMAX and 
Gamma filterings, could also be considered as special cases of flexible state-space 
models. 
2 Formulation of Problem 
Assume that the source signals are a stationary zero-mean i.i.d processes and mutu- 
ally statistically independent. Let s(t) = (s (t),..., sn(t)) be an unknown vector of 
independent i.i.d. sources. Suppose that the mixing model is described by a stable 
linear state discrete-time system 
(k + 1) = A(k) + Bs(k) + Lp(k), (1) 
u(k) = c(k) + Ds() + (2) 
where   R r is the state vector of system, s(k)  R n is the vector of source signals 
and u(k)  R ' is the vector of sensor signals. A, B, C and D are the mixing 
matrices of the state space model with consistent dimensions. p(k) is the process 
noise and O(k) is sensor noise of the mixing system. If we ignore the noise terms 
in the mixing model, its transfer function matrix is described by a m x n matrix of 
the form 
H(z) = U(zI - + (3) 
where z - is a delay operator. 
We formulate the blind separation problem as a task to recover original signals 
from observations u(t) without prior knowledge on the source signals and the state 
space matrices [A, B, C, D] besides certain statistic features of source signals. We 
propose that the demixing model here is another linear state-space system, which 
is described as follows, (see Fig. 1) 
x(k + 1) = Ax(k) + Bu(k) + LR(k), (4) 
= cx() + (5) 
where the input u(k) of the demixing model is just the output (sensor signals) 
of the mixing model and the �R(k) is the reference model noise. A, B, C and 
D are the demixing matrices of consistent dimensions. In general, the matrices 
W = [A, B, C, D, L] are parameters to be determined in learning process. 
For simplicity, we do not consider, at this moment, the noise terms both in 
the mixing and demixing models. The transfer function of the demixing model 
is W(z) = C(zI - A)-B + D. The output y(k) is designed to recover the source 
signals in the following sense 
(6) 
650 L. Zhang and A. Cichocld 
� 
Refe,'nces (k) 
+ y(k) 
Figure 1: General state-space model for blind deconvolution 
where P is any permutation matrix and A(z) is a diagonal matrix with iz -Ti 
in diagonal entry (i,i), here hi is a nonzero constant and ri is any nonnegative 
integer. It is easy to see that the linear state space model mixture is an extension 
of instantaneous mixture. When both the matrices A, B, C in the mixing model 
and A, B, C in the demixing model are null matrices, the problem is simplified to 
standard ICA problem [1-8]. 
The question here is whether exist matrices [A, B, C, D] in the demixing model (4) 
and (5), such that its transfer function W(z) satisfies (6). It is proven [12] that if 
the matrix D in the mixing model is of full rank, rank(D) = n, then there exist 
matrices [A, B, C, D], such that the output signal y of state-space system (4) and 
(5) recovers the independent source signal s in the sense of (6). 
3 Learning Algorithm 
Assume that p(y, W), Pi (Yi, W) are the joint probability density function of y and 
marginal pdf of yi, (i -- 1,. �., n) respectively. We employ the mutual information 
of the output signals, which measures the mutual independence of the output signals 
yi(k), as a risk function [1,2] 
l(W) = -H(y, W) q- y H(yi, W), (7) 
where 
H(y, W) = - f p(u, W) logp(y, W)dy, H(yi, W) = -/pi(yi, W)logPi(yi, W)dyi. 
In this paper we do not directly develop learning algorithms to update all param- 
eters W = [A, B, C, D] in demixing model. We separate the blind deconvolution 
problem into two procedures: separation and state-estimation. In the separation 
procedure we develop a novel learning algorithm, using a new search direction, to 
update the matrices C and D in output equation (5). Then we define a hidden 
innovation of the output and use Kalman filter to estimate the state vector x(k). 
For simplicity we suppose that the matrix D in the demixing model (5) is nonsin- 
gular n x n matrix. From the risk function (7), we can obtain a cost function for 
on line learning 
1 n 
l(y, W) = -3 log det(D T D)) - y 1ogpi(Yi, W), (8) 
i=1 
Blind Separation of Filtered Sources 651 
where det(DTD) is the determinant of symmetric positive definite matrix DTD. 
For the gradient of I with respect to W, we calculate the total differential dl of 
l(y, W) when we takes a differential dW on W 
at(u, w) = t(u, w + dw) - t(u, w). (9) 
Following Amari's derivation for natural gradient methods [1-3], we have 
dl(y, W) = -tr(dDD -i) + qT(y)dy, (10) 
where tr is the trace of a matrix and q(y) is a vector of nonlinear activation 
functions 
qi(Yi) = dlogpi(Yi)_ P'i(Yi) 
-- dyi - Pi(Yi)' (11) 
Taking the derivative on equation (5), we have following approximation 
dy = dCx(k) + dDu(k). (12) 
On the other hand, from (5), we have 
u(k) - D-l(y(k)-Cx(k)) (13) 
Substituting (13) into (12), we obtain 
dy = (dC - dDD-1C)x + dDD-ly. (14) 
In order to improve the computing efficiency of learning algorithms, we introduce a 
new search direction 
dX = dC-dDD-1C, (15) 
dX2 = dDD -1. (16) 
Then the total differential dl can be expressed by 
dl = -tr(dX2) + qT(y)(dX13 + dX2y). (17) 
It is easy to obtain the derivatives of the cost function I with respect to matrices 
X1 and X2 as 
O1 
= q(y(k))xT(k), (18) 
Ol 
= qo(u(k))uT(k)- I. (19) 
OX2 
From (15) and (16), we derive a novel learning algorithm to update matrices (7 and 
D. 
AC(k) -- r/(-q(y(k))xr(k) + (I- q(y(k))y'(k))(7(k)), (20) 
AD(k) =  (I- T(y(k))yr(k)) D(k). (21) 
The equilibrium points of the learning algorithm satisfy the following equations 
E[(y(k))xr(k)] = O, (22) 
E - = 0. (23) 
This means that separated signals y could achieve as mutually independent as 
possible if the nonlinear activation function q(y) are,suitably chosen and the state 
vector x(k) is well estimated. From (20) and (21), we see that the natural gradient 
learning algorithm [2] is covered as a special case of the learning algorithm when 
the mixture is simplified to instantaneous case. 
The above derived learning algorithm enable to solve the blind separation problem 
under assumption that state matrices A and B are known or designed appropriately. 
In the next section instead of adjusting state matrices A and B directly, we propose 
new approaches how to estimate state vector x. 
652 L. Zhang and A. Cichocki 
4 State Estimator 
From output equation (5), it is observed that if we can accurately estimate the state 
vector x(k) of the system, then we can separate mixed signals using the learning 
algorithm (20) and (21). 
4.1 Kalman Filter 
The Kalman filter is a useful technique to estimate the state vector in state-space 
models. The function of the Kalman Filter is to generate on line the state estimate 
of the state x(k). The Kalman filter dynamics are given as follows 
x(k 
