602 
AUTOMATIC LOCAL ANNEALING 
Jared Leinbach 
Depamnent of Psychology 
Carnegie-Mellon University 
Pittsburgh, PA 15213 
ABSTRACT 
This research involves a method for finding global maxima 
in constraint sa.fi. sfacfion networks. I.t is an annea.h.'..'ng 
process but, unlike most others, reqmres no annealing 
schedule. Temperature is instead determined locally by 
units at each update, and thus all processing is done at the 
unit level. There are two major practical benefits to 
processing this way: 1) processing can continue in 'bad' 
areas of the network, while 'good' areas remain stable, and 
2) processing continues in the 'bad' areas, as long as the 
constraints remain poorly satisfied (i.e. it does not stop 
after some predetermined number of cycles). As a result, 
this method not only avoids the kludge of requiring an 
externally determined annealing schedule, but it also finds 
global maxima more quickly and consistently than 
externally scheduled systems (a comparison to the 
Boltzmann machine (Ackley et al, 1985) is made). Finally, 
implementation of this method is computationally trivial. 
INTRODUCTION 
A constraint satisfaction network, is a network whose units represent hypolheses, 
between which there are various constraints. These consuaints are represented by bi- 
directional connections between the units. A positive connection weight suggests that if 
one hypothesis is accepted or rejected, the other one should be also, and a negative 
connection weight suggests that if one hypothesis is accepted or rejected, the other one 
should not be. The relative importance of satisfying each constraint is indicated by the 
absolute size of the corresponding weight. The acceptance or rejection of a hypothesis is 
indicated by the activation of the corresponding unit. Thus every point in the activation 
space corresponds to a possible solution to the constraint problem represented by the 
network. The quality of any solution can be calculated by summing the 'satisfiedn.ess' of 
all the constraints. The goal is to find a point in the activation space for which the quality 
is at a maximum. 
Automatic Local Annealing 603 
Unfortunately, if units update deterministically (i.e. if they always move toward the state 
that best satisfies heir conswaints) lhere is no means of avoiding local quality maxima in 
the activation space. This is simply a fundamental problem of all gradient decent 
procedures. Annealing systems attempt to avoid this problem by always giving units 
some probability of not moving towards the slate that best satisfies lheir constraints. This 
probability is called he 'temperature' of the network. When he teanure is high, 
solutions are generally not good, but the network moves easily throughout the activation 
space. When lhe temperature is low, lhe network is committed to one area of the 
activation space, but it is very good at improving its solution within that area. Thus the 
annealing analogy is born. The notion is that ff you start wilh the teanperature high, and 
lower it slowly enough, the network will gradually replace its 'state mobility' with 'state 
improvement ability', in such a way as to guide itseft into a globally maximal state (much 
as the atoms in slowly annealed metals find optimal bonding structures). 
To search for solutions this way, requires some means of determining a temperature for 
the network, at every uo_e. Annealing systems simply use a predetermined schedule to 
provide this information. However, there are both practical and lbeoretical problems with 
this approach. The main practical problems are the following: 1) once an annealing 
schedule comes to an end, all processing is finished regardless of the quality of the 
current solution, and 2) temperature must be uniform across the network, even though 
different parts of the network may merit different temperatures (this is the case any time 
one part of the network is in a 'betmr' area of the activation space than another, which is 
a natural condition). The theoretical problem with this approach involves the selection of 
annealing schedules. In order to pick an appropriate schedule for a network, one must 
use some knowledge about what a good solution for that network is. Thus in order to get 
the system to find a solution, you must already know something about the solution you 
want it to find. The problem is that one of the most critical elements of the process, the 
way that the temperature is decreased, is handled by something other than the network 
itself. Thus the quality of the final solution must depend, at least in part, on that system's 
understanding of the problem. 
By allowing each unit to control its own temperature during processing, Automatic Local 
Annealing avoids this serious kludge. In addition, by resolving the main practical 
problems, it also ends up finding global maxima more quickly and reliably than 
externally controlled systems. 
MECHANICS 
All units take on continuous activations between a uniform minimum and maximum 
value. There is also a uniform resting activation for all units (between the minimum and 
maximum). Units start at random activations, and are updated synchronously at each 
cycle in one of two possible ways. Either they are updated via any ordinary update rule 
for which a positive net input (as defined below) increases activation and a negative net 
input decreases activation, or they are simply reset to their resting activation. There is an 
update probability function that determines the probability of normal update for a unit 
based on its temperature (as defined below). It should be noted that once the net input for 
604 Leinbach 
a unit has been calculated, f'mding its tem is trivial 0he quantity (a i - rest) in the 
equation for goodness i can come outside the summatkm). 
netinput i = j(aj--rest) x wii 
temper atur e i = -g ��dnes s i / max s gdnss i 
goodness i / ggdnss i 
ff goodss > 0 
othcrse 
goodness. = .(arest) x w..x (a.,--resO 
maxposgdnss i = the largest pos. value that goodness i could be 
maxneggdnss i = the largest neg. value that goodness i could be 
Maxposgdnss and maxneggdnss are constants that can be calculated once for each unit at 
the beginning of simulation. They depend only on hc weights into the unit, and the 
constant maximum, minimum and resting activation values. Temperature is always a 
value between 1 and -1, with 1 representing high temperature and -1 low. 
SIMULATIONS 
The param below were used in processing both of hc networks that were tested. 
The fu-st network processed (Figure la) has two local maxima that are extremely close to 
its two global maxima. This is a very 'difficult' network in he sense that he search for a 
global maximum must be extremely sensitive to he minute difference between he global 
maxima and he next-best local maxima. The other network processed (Figure lb) has 
many local maxima, but none of them are especially close to the global maxima. This is 
an 'easy' network in the sense that the slow and cautious process that was used, was not 
really necessary. A more appropriate set of parameters would have improved 
performance on this second network, but it was not usexl in order to illustrate the relative 
generality of he algorithm. 
Parameters: 
maximum activation = 1 
minimum activation = 0 
resting activation = 0.5 
normal update rule: 
A activation i = netinput i x (maxactivation - activationi) x k 
netinput i x (activation i - minactivation) x k 
ff netinput i > o 
otherwise 
wih'k = 0.6 
Automatic Local Annealing 605 
I 
-1 
-.79 0 1 
TEMPERATLq,E 
This function defines a process that moves slowly towards a global maximum, moves 
away from even good solutions easily, and 'freezes' units that are colder than -0.79. 
RESULTS 
The results of running lhe Automatic Local Annealing process on these two networks (in 
comparison to a standard Boltzmann Machine's pedormame) are summarized in figures 
2tl and 2b. Wilh Automatic Local Annealing (ALA), lhe probability of having found a 
stable global maximum departs from zero fairly soon after processing begins, and 
increases smoothly up to one. The Boltzmann Machine, instead, makes little 'useful' 
prog!v. as until lhe end of the annealing schedule, and then quickly moves into a solution 
which may or may not be a global maximum. In order to get its reliability near that of 
ALA, the Boltzmann Machine's schedule must be so slow that solutions are found much 
more slowly than ALA. Conversely in order to start finding solution as quickly as ALA, 
such a short schedule is necessary that the reliability becomes much worse than ALA's. 
Finally, if one makes a more reasonable comparison to the Boltzmann Machine (either by 
changing the parameters of the ALA process to maximize its performance on each 
network, or by using a single annealing schedule with the Boltzmann Machine for both 
networks), the overall performance advantage for ALA increases substantially. 
DISCUSSION 
HOW IT WORKS 
The characteristics of the approach to a global maximum are determined by the shape of 
the update probability function. By modifying this shape, one can control such things as: 
how quickly/steadily the network moves towards a global maximum, how easily it moves 
away from local maxima, how good a solution must be in order for it to become 
completely stable, and so on. The only critical feature of the function, is that as 
temperature decreases the probability of normal update increases. In this way, the colder 
a unit gets the more steadily jt progresses towards an extreme activation value, and the 
holier a unit gets the more lime it ds near resting activation. From this you get hot 
606 Leinbach 
-2.5 
1 1 
1 1 
-2.5 -2.5 -2.5 
-2.5 -2.5 -2.5 
1 1 
1 
1 1 1 
-2.5 
Figure la. A 'Difficult' Network. 
Global maxima are: 1) all eight upper units on, wih the remaining units off, 2) all eight 
lower units on with the remaining units off. Nex,t b
