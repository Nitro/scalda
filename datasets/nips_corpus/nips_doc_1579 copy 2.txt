General Bounds on Bayes Errors for 
Regression with Gaussian Processes 
Manfred Opper 
Neural Computing Research Group 
Dept. of Electronic Engineering 
and Computer Science, 
Aston University, 
Birmingham, B4 7ET 
United Kingdom 
oppermast on. ac. uk 
Francesco Vivarelli 
Centro Ricerche Ambientali 
Montecatini, 
via Ciro Menotti, 48 
48023 Marina di Ravenna, 
Italy 
fvivarellicramont. it 
Abstract 
Based on a simple convexity lemma, we develop bounds for differ- 
ent types of Bayesian prediction errors for regression with Gaussian 
processes. The basic bounds are formulated for a fixed training set. 
Simpler expressions are obtained for sampling from an input distri- 
bution which equals the weight function of the covariance kernel, 
yielding asymptotically tight results. The results are compared 
with numerical experiments. 
I Introduction 
Nonparametric Bayesian models which are based on Gaussian priors on function 
spaces are becoming increasingly popular in the Neural Computation Community 
(see e.g.[2, 3, 4, 7, 1]). Since the model classes considered in this approach are 
infinite dimensional, the application of Vapnik - Chervonenkis type of methods to 
determine bounds for the learning curves is nontrivial and has not been performed 
so far (to our knowledge). In these methods, the target function to be learnt is 
fixed and input data are drawn independently at random from a fixed (unknown) 
distribution. The approach of this paper is different. Here, we assume that the target 
is actually drawn at random from a known prior distribution, and we are interested 
in developing simple bounds on the average prediction performance (with respect 
to the prior) which hold for a fixed set of inputs. Only at a later stage, an average 
over the input distribution is made. 
General Bounds on Bayes Errors for Regression with Gaussian Processes 303 
2 Regression with Gaussian processes 
To explain the Gaussian process scenario for regression problems [4], we assume that 
observations y  R at input points x  R D are corrupted values of a function O(x) 
by an independent Gaussian noise with variance a 2. The appropriate stochastic 
model is given by the likelihood 
_ (-o()) 2 
e 2r2 
po(ylx) = (1) 
V/2rcr 2 
The goal of a learner is to give an estimate of the function 0(x), based on a set of 
observed example data D, = ((x, y),..., (,, Yt)). As the prior information about 
the unknown function 0() we asume that 0 is a realization of a Gaussian random 
field with zero mean and covariance 
C(, ') = ,[0�)0(')]. (2) 
It is useful to expand the random functions as 
= wkOk() (a) 
in a complete set of deterministic functions b (z) with random Gaussian coefficients 
w. As is well known, if the b are chosen as orthonormal eigenfunctions of the 
integral equation 
C(x,x')qb(x')p(x')dx' = Akb(x), (4) 
with eigenvalues A and a nonnegative weight function p(x), the a priori statistics 
of wt is simple. They are independent Gaussian variables which satisfy lg[wwt] = 
3 Prediction and Bayes error 
Usually, the posterior mean of O(x) is chosen as the prediction O(x) on a new point 
x based on a dataset Dn = (x,y),... ,(xn,yn). Its explicit form can be easily 
derived by using the expansion (x) = y. tbb (x), and the fact that for Gaussian 
random variables, their mean coincides with their most probable value. Maximizing 
the log posterior, with respect to the w, one finds for the infinite dimensional vector 
  (w)=o .....  the result  = (621 + AV) - b where Vt = i k(Xi)l(xi) 
At = A8t and b = i Ayi(zi) Fixing the set of inputs , the Bayesian 
prediction error at a point � is given by 
Evaluating (5) yields, after some work, the expression 
e(zlz ) = a2 { (a2I + AV) -1 AS(z)} (6) 
1 n 
with the matrix Ut(z) = O(z)Ot(z). U has the properties that  Ei= U(zi) = V 
and f dz p(z)U(x) = I. We define the Bayesian training eor as the empirical 
average of the error (5) at the n datapoints of the training set and the Bayesian 
generalization eor as the average error over all z weighted by the function p(z). 
We get 
= k (I + } (7) 
= {a + (8) 
304 M. Opper and F. lh'varelli 
4 Entropic error 
In order to understand the next type of error [9], we assume that the data arrive 
sequentially, one after the other. The predictive distribution after t - 1 training data 
at the new input xt is the posterior expectation of the likelihood (1), i.e. 
P(ylxt,Ot-) - E[Po(y[xt)lOt-x]. 
Let Lt as the Bayesian average of the relative entropy (or Kullback Leibler diver- 
gence) between the predictive distribution and the true distribution Po from which 
the data were generated, i.e. Lt = 1F, [DKi_, (P011/b)]. It can also be shown that 
Lt =  2 . Hence, when the prediction error is small, we will have 
(9) 
The cumulative entropic error Ee (x'*) is defined by summing up all the losses (which 
gives an integrated learning curve) from t = I up to time n and one can show that 
E(x,) =  Lt(xt,Dt-) = lgD:L (PIIP'*) =  Trln (I + AV/a 2) (10) 
where P l-Ii= Po(Yil'xi) and P = IF,  . 
=  [1-Ii= Po(yilx)] The first equality may be 
found e.g. in [9], and the second follows from direct calculation. 
5 Bounds for fixed set of inputs 
In order to get bounds on (7),(8) and (10), we use a lemma, which has been used in 
Quantum Statistical Mechanics to get bounds on the free energy. The lemma (for 
the special function f(x) = e -x) was proved by Sir Rudolf Peierls in 1938 [10]. In 
order to keep the paper self contained, we have included the proof in the appendix. 
Lemma 1 Let H be a real symmetric matrix and f a convex real function. Then 
Wr f(t) _> k f(tk). 
By noting, that for concave functions the bound goes in the other direction, we 
immediately get 
a 2 ,X V _< a2 y. a 2 ,Xk v (11) 
 k k 
a2A   a2 a2A (12) 
k k 
1 
I ln (1 + VA/a 2) <  ln (1 + nvA/ 2) (13) 
() 5  - 
k k 
where in the rightmost inequalities, we assume that all n inputs are in a compact 
region Z), and we define v = supxv b} (x).  
The entropic case may also be proved by Hadamard's inequality. 
General Bounds on Bayes Errors for Regression with Gaussian Processes 305 
6 Average case bounds 
Next, we assume that the input data are drawn at random and denote by (...) 
the expectations with respect to the distribution. We do not have to assume inde- 
pendence here, but only the fact that all marginal distributions for the n inputs are 
identical. t Using Jensen's inequality 
(16) 
Akuk (14) 
k 
0.2k (15) 
k 
1 
{E(xn)}   ln (1 + nuA/a 2) 
k 
where now u = (;b(x)). This result is especially simple, when the weighting 
function p(x) is a probability density and the inputs have the marginal distribution 
p(x). In this case, we simply have u - 1. In this case, training and generalization 
error sandwich the bound 
A (17) 
b -- 0'2  0' 2 q_ nk 
k 
We expect that the bound eb becomes asymptotically exact, when n -+ oo. This 
should be intuitively clear, because training and generalization error approach each 
other asymptotically. This fact may also be understood from (9), which shows that 
 asymptotically equal to the 
the cumulative entropic error is within a factor of  
cumulative generalization error. By integrating the lower bound (17) over n, we 
obtain precisely the upper bound on E with a factor 2, showing that upper and 
lower bounds show the same behaviour. 
7 Simulations 
We have compared our bounds with simulations for the average training error and 
generalization error for the case that the data are drawn from p(x). Results for the 
entropic error will be given elsewhere. 
We have specialized on the case, where the covariance kernel is of the RBF form 
C(x,x') = exp[(x - Xt)2//2], and p(x) = (2r) }e � z2, for which, following Zhu 
et al. (1997), the k-th eigenvalue of the spectrum (k = 0... oo) can be written 
as ,k = ab , where a = x/Z, b = c/,k 2, c = 2 1 + 2/A2+ V/1 + 4/X 2 , and X 
is the lengthscale of the process. We estimated the average generalisation error 
for each training set based on the exact analytical expressions (8) and (7) over 
the distribution of the datasets by using a Monte Carlo approximation. To begin 
with, let us consider x 6 R. We sampled the 1-dimensional input space generating 
100 training sets whose data points were normally distributed around zero with 
unit variance. For each generation, the expected training and generalisation errors 
for a GP have been evaluated using up to 1000 data points. We set the value 
of the lengthscale 2 A to 0.1 and we let the noise level a 2 assume several values 
(if2 = 10-4, 10-a, 10-2, 10-1, 1). Figure 1 shows the results we obtained when 
'The value of the lengthscale A has the effect of stretching the training and learning 
curves; thus the results of the experiments performed with different A are qualitatively 
similar to those presented. 
306 M. Opper and E lq'varelli 
0.01 
0.1 
lO lOO lOOO 1 
n 
(a) A = 0.1, a 2 = 0.1 
10 100 1000 
n 
(b) A=0.1, a z=l 
Figure 1: The Figures show the graphs of the training and learning curves with their 
bound b(n) obtained with A = 0.1; the noise level is set to 0.1 in Figure l(a) and to 
1 in Figure l(b). In all the graphs,  and a(n) are drawn by the solid line and their 
95% confidence interval is signed by the dotted curves. The bound b(n) is drawn by the 
dash-dotted lines. 
a 2 = 0.1 (Figure l(a)) and a 2 = 1 (Figure l(b)). The bound e,(n) lies within the 
training and learning curves, being an upper bound for et (n) and a lower bound for 
e a (n). This bound is tighter for the processes with higher noise level; in particular, 
for large datasets the error bars on the curves et(n) and eg(n) overlap the bound 
e,(n). The curves et(n), eg(n) and e,(n) approach zero as O(log(n)/n). 
Our bounds can also be applied to higher dimensions D > I using the covariance 
C(x,x'): exp (-I[x - x'll2/A 2) (18) 
for x, x'  R D. Obviously the integral kernel C is just a di
