An Analysis of Turbo Decoding 
with Gaussian Densities 
Paat Rusmevichientong and Benjamin Van Roy 
Stanford University 
Stanford, CA 94305 
( paatrus, bvr) stanf ord. edu 
Abstract 
We provide an analysis of the turbo decoding algorithm (TDA) 
in a setting involving Gaussian densities. In this context, we are 
able to show that the algorithm converges and that - somewhat 
surprisingly - though the density generated by the TDA may differ 
significantly from the desired posterior density, the means of these 
two densities coincide. 
I Introduction 
In many applications, the state of a system must be inferred from noisy observations. 
Examples include digital communications, speech recognition, and control with in- 
complete information. Unfortunately, problems of inference are often intractable, 
and one must resort to approximation methods. One approximate inference method 
that has recently generated spectacular success in certain coding applications is the 
turbo decoding algorithm [1, 2], which bears a close resemblance to message-passing 
algorithms developed in the coding community a few decades ago [4]. It has been 
shown that the TDA is also related to well-understood exact inference algorithms 
[5, 6], but its performance on the intractable problems to which it is applied has 
not been explained through this connection. 
Several other papers have further developed an understanding of the turbo decoding 
algorithm. The exact inference algorithms to which turbo decoding has been related 
are variants of belief propagation [7]. However, this algorithm is designed for in- 
ference problems for which graphical models describing conditional independencies 
form trees, whereas graphical models associated with turbo decoding possess many 
loops. To understand the behavior of belief propagation in the presence of loops, 
Weiss has analyzed the algorithm for cases where only a single loop is present Ill]. 
Other analyses that have shed significant light on the performance of the TDA in 
its original coding context include [8, 9, 10]. 
In this paper, we develop a new line of analysis for a restrictive setting in which un- 
derlying distributions are Gaussian. In this context, inference problems are tractable 
and the use of approximation algorithms such as the TDA are unnecessary. How- 
ever, studying the TDA in this context enables a streamlined analysis that generates 
new insights into its behavior. In particular, we will show that the algorithm con- 
verges and that the mean of the resulting distribution coincides with that of the 
576 P Rusmevichientong and B. V. Roy 
desired posterior distribution. 
While preparing this paper, we became aware of two related initiatives, both in- 
volving analysis of belief propagation when priors are Gaussian and graphs possess 
cycles. Weiss and Freeman [12] were studying the case of graphs possessing only 
cliques of size two. Here, they were able to show that, if belief propagation con- 
verges, the mean of the resulting approximation coincides with that of the true 
posterior distribution. At the same time, Frey [3] studied a case involving graphical 
structures that generalize those employed in turbo decoding. He also conducted an 
empirical study. 
The paper is organized as follows. In Section 2, we provide our working definition 
of the TDA. In Section 3, we analyze the case of Gaussian densities. Finally, a 
discussion of experimental results and open issues is presented in Section 4. 
2 A Definition of Turbo Decoding 
Consider a random variable x taking on values in n distributed according to a 
density P0. Let Yl and Y2 be two random variables that are conditionally indepen- 
dent given x. For example, yl and y2 might represent outcomes of two independent 
transmissions of the signal x over a noisy communication channel. If y and y2 are 
observed, then one might want to infer a posterior density f for x conditioned on 
y and y2. This can be obtained by first computing densities p and p, where the 
first is conditioned on y and the second is conditioned on y2. Then, 
where ct is a normalizing operator defined by 
g 
.g _-- f 
and multiplication/division are carried out pointwise. 
Unfortunately, the problem of computing f is generally intractable. The computa- 
tional burden associated with storing and manipulating high-dimensional densities 
appears to be the primary obstacle. This motivates the idea of limiting attention 
to densities that factor. In this context, it is convenient to define an operator r 
that generates a density that factors while possessing the same marginals as another 
density. In particular, this operator is defined by 
n 
= H f ^ 
i=1 ' [ i=ai ) 
for all densities g and all a  n, where d A di = dl. di-ldi+l 'dn. 
One may then aim at computing f  a proxy for f. Unfortunately, even this 
problem is generally intractable. The TDA can be viewed  an iterative algorithm 
for approximating f. 
Let operators F and F2 be defined by 
k p0/ 
and 
An Analysis of Turbo Decoding with Gaussian Densities 577 
for any density g. The TDA is applicable in cases where computation of these two 
operations is tractable. The algorithm generates sequences q?) and q?) according 
to 
q?+l)= Flq?) and 
initialized with densities q0) and q? that factor. The hope is that a(q?)q?)/po) 
converges to an approximation of rf. 
3 The Gaussian Case 
We will consider a setting in which joint density of x, Yl, and y2, is Gaussian. In this 
context, application of the TDA is not warranted - there are tractable algorithms for 
computing conditional densities when priors are Gaussian. Our objective, however, 
is to provide a setting in which the TDA can be analyzed and new insights can be 
generated. 
Before proceeding, let us define some notation that will facilitate our exposition. 
We will write g - N(g, Eg) to denote a Gaussian density g whose mean vector e/nd 
covariance matrix are  and E, respectively. For any matrix A, 5(A) will denote 
a diagonal matrix whose entries are given by the diagonal elements of A. For any 
diagonal matrices X and Y, we write X _< Y if Xii _< Y/i for all i. For any pair of 
nonsingular covariance matrices Eu and E,such that y,l+ EI _ I is nonsingular, 
let a matrix A,r,v be defined by 
Ar,,r,v  (y,l + EI _ i)-1. 
To reduce notation, we will sometimes denote this matrix by Au, 
When the random variables x, Yl, and y: are jointly Gaussian, the densities p, 
f, and p0 are also Gaussian. We let 
and assume that both E1 and E: are symmetric positive definite matrices. We will 
also assume that P0  N(0, I) where I is the identity matrix. It is easy to show 
that Ar,,r, is well-defined. 
The following lemma provides formulas for the means and covariances that arise 
from multiplying and rescaling Gaussian densities. The result follows from simple 
algebra, and we state it without proof. 
Lemma 1 Let u  N(, E,,) and v ,, N(, E), where E,, and E are positive 
definite. If E 1 + E 1 - I is positive definite then 
One immediate consequence of this lemma is an expression for the mean of f: 
-1 
 = Az,z (E{11 + E 2). 
Let ,5 denote the set of covariance matrices that are diagonal and positive definite. 
Let  denote the set of Gaussian densities with covariance matrices in $. We then 
have the following result, which we state without proof. 
Lemma 2 The set  is closed under F1 and F. 
If the TDA is initialized with q0), q(0)  G, this lemma allows us to represent all 
iterates using appropriate mean vectors and covariance matrices. 
578 P Rusmevichientong and B. V. Roy 
3.1 Convergence Analysis 
Under suitable technical conditions, it can be shown that the sequence of mean 
vectors and covariance matrices generates by the TDA converges. Due to space 
limitations, we will only present results pertinent to the convergence of covariance 
matrices. Furthermore, we will only present certain central components of the 
analyses. For more complete results and detailed analyses, we refer the reader to 
our upcoming full-length paper. 
Recall that the TDA generates sequences q?) and q?) according to 
q(+)= Fq? ) and q?+)- F2q? ) 
I m � 
As discussed earlier, if the algorithm is initialized with elements of G, by Lemma 2, 
for appropriate sequences of mean vectors and covariance matrices. It turns out 
that there are mappings : 3 + 3 and : 3 -+ 3 such that 
 = T E(2 and = , 
for all k. Let T = T o T. To establish convergence of E? ) and E? ), it suffices to 
show that Tn(? )) converges. The following theorem establishes this and further 
points out that the limit does not depend on the initial iterates. 
Theorem 1 There exists a matrix V*  3 such that 
lim Tn(V) = V*, 
n-}o<) 
for all V  3. 
3.1.1 Preliminary Lemmas 
Our proof of Theorem 1 relies on a few lemmas that we will present in this section. 
We begin with a lemma that captures important abstract properties of the function 
7-. Due to space constraints, we omit the proof, even though it is nontrivial. 
Lemma 3 
(a) There exists a matrix D  3 such that for all D  3, D < T(D) < I. 
(b) For all X, Y  3, if X < Y then T(X) < T(Y). 
(c) The function T is continuous on 3. 
(d) For all   (0, 1) and D  3, (1 + a)T (D) < T (D) for some a > O. 
The following lemma establishes convergence when the sequence of covariance ma- 
trices is initialized with the identity matrix. 
Lemma 4 The sequence T n (I) converges in 3 to a fixed point of T. 
Proof: By Lemma 3(a), 7-(1) <_ I, and it follows from monotonicity of 7- (Lemma 
3(b)) that <_ 7-n(i) for all n. Since 7-(I) is bounded below by a matrix 
D  3, the sequence converges in 3. The fact that the limit is a fixed point of 7- 
follows from the continuity of 7- (Lemma 3(c)). � 
Let V* = lim__. T  (I). This matrix plays the following special role. 
Lemma 5 The matrix V* is the unique fixed point in 3 of T. 
An Analysis of Turbo Decoding with Gaussian Densities 579 
Proof: Because T n (I) 
