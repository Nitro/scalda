Self-organisation in real 
Anti-Hebb in 'Channel 
neurons: 
Space'? 
Anthony J. Bell 
AI-lab, 
Vrije Universiteit Brussel 
Pleinlaan 2, B-1050 Brussels 
BELGIUM, (tony@arti.vub.ac.be) 
Abstract 
Ion channels are the dynamical systems of the nervous system. Their 
distribution within the membrane governs not only communication of in- 
formation between neurons, but also how that information is integrated 
within the cell. Here, an argument is presented for an 'anti-Hebbian' rule 
for changing the distribution of voltage-dependent ion channels in order 
to fiatten voltage curvatures in dendrites. Simulations show that this rule 
can account for the self-organisation of dynamical receptive field properties 
such as resonance and direction selectivity. It also creates the conditions 
for the faithful conduction within the cell of signals to which the cell has 
been exposed. Various possible cellular implementations of such a learn- 
ing rule are proposed, including activity-dependent migration of channel 
proteins in the plane of the membrane. 
I INTRODUCTION 
1.1 NEURAL DYNAMICS 
Neural inputs and outputs are temporal, but there are no established ways to think 
about temporal learning and dynamical receptive fields. The currently popular sim- 
ple recurrent nets have only one kind of dynamical component: a capacitor, or time 
constant. Though it is possible to create any kind of dynamics using capacitors and 
static non-linearities, it is also possible to write any program on a Turing machine. 
59 
60 Bell 
Biological evolution, it seems, has elected for diversity and complexity over unifor- 
mity and simplicity in choosing voltage-dependent ion channels as the 'instruction 
set' for dynamical computation. 
1.2 ION CHANNELS 
As more ion channels with varying kinetics are discovered, the question of their 
computational role has become more pertinent. Figure 1, derived from a model 
thalamic cell, shows the log time constants of 11 currents, plotted against the voltage 
ranges over which they activate or inactivate. The variety of available kinetics is 
probably under-represented here since a combinal.orial number of differences can be 
obtained by combining different protein sub-domains to make a channel [6]. 
Given the likelihood that channels are inhomogenously distributed throughout the 
dendrites [7], one way to tackle the question of their computational role is to search 
for a self-organisational principle for forming this distribution. Such a 'learning 
rule' could be construed as operating during development or dynamically during 
the life of an organism, and could be considered complementary to learning involv- 
ing synaptic changes. The   _ 
resulting distribution and mix  i   t 
of channels would then be, in  - /r--..,, ,', 
some sense, optimal for integrat- 
ing and communicating the par- 
ticular high-dimensional spatio- 
temporal inputs which the cell 
was accustomed to receiving. 
Figure 1: Diversity of ion chan- 
nel kinetics. The voltage- 
dependent equilibrium log time 
constants of 11 channels are plot- 
ted here for the voltage ranges 
for which their activation (or 0- 
inactivation) variables go from 
0.1  0.9 (or 0.9  0.1). The 
channel kinetics are taken from a 
model by W.Lytton [10]. Notice to-, 
the range of speeds of operation 
from the spiking Na + channel 
around 0.1ms, to the KM chan- 
nel in the ls (cognitive) range. 
-- CaT artact. 
AR  inact. 
' Kc [Ca :+ } = 5OhM 
- CaTact'/ Ndlaacl. X< ..... 
Nap 
� Na act. 
2 
Membrane potential (mV) 
THE BIOPHYSICAL SUBSTRATE 
The substrate for self-organisation is the standard cable model for a dendrite or 
a,xon; 
02V = C OV 
Ox ot + - + (1) 
j  
Anti-Hebb in 'Channel Space'? 61 
In this Ga represents the conductance along the axis of the cable, 6' is the capac- 
itance and the two sums represent synaptic (indexed by j) and intrinsic (indexed 
by k) currents. G is a maximum conductance (a channel density or 'weight'), # is 
the time-varying fraction of the conductance active, and E is a reversal potential. 
The system can be summarised by saying that the current flow out of a segment of 
a neuron is equal to the sum of currents input to that segment, plus the capacitive 
charging of the membrane. 
This leads to a simpler form' 
Here, 
i= Eyiij + y. y}i} (2) 
i = 02V/Ox 2,  = j/G,, i I = g1(V-E1) and 6' is considered as an intrinsic 
conductance whose y} and i} are C/G, and OV/Ot respectively. In this form, it 
is more clear that each part of a neuron can be considered as a 'unit', diffusively 
coupled to its neighbours, to which it passes its weighted sum of inputs. The weights 
excitator� inhibitory leakage 
channels channels .hannels 
capacitive 
membrane 
synaptic charging 
channels 
Figure 1' 
cable equation is just Kirchoff's Law: 
Electro- 
diffusive 
spread 
utside 
A compartment of a neuron, shown schematically and as a circuit. 
current in -- current out 
The 
y}, representing the Ga-normalised densities of channel species k, are considered to 
span channel space, as opposed to the y: weights which are our standard synaptic 
strength parameters. Parameters determining the dynamics of g's specify points 
in kinetics space. Neuromodulation [8], a universally important phenomenon in real 
nervous systems, consists of specific chemicals inducing short-term changes in the 
kinetics space co-ordinates of a channel type, resulting, for example, in shifts in the 
curves in Figure 1. 
3 THE ARGUMENT FOR ANTI-HEBB 
Learning algorithms, of the type successful in static systems, have not been con- 
sidered for these low-level dynamical components (though see [2] for approaches to 
synaptic learning in realistic systems). Here, we address the issue of unsupervised 
learning for channel densities. In the neural network literature, unsupervised learn- 
ing consists of Hebbian-type algorithms and information theoretic approaches based 
on objective functions [1]. In the absence of a good information theoretic frame- 
work for continuous time, non-Gaussian analog systems where noise is undefined, 
we resort to exploring the implications of the effects of simple local rules. 
62 Bell 
The most obvious rule following from equation 2 would be a correlational one of 
the following form, with the learning rate e positive or negative: 
Ay = eii (3) 
While a volarising (or Hebbian) rule (see Figure 3) makes sense for synaptic chan- 
nels as an a method for amplifying input signals, it makes less sense for intrinsic 
channels. Were it to operate on such channels, statistical fluctuations from the 
uniform channel distribution would give rise to self-reinforcing 'hot-spots' with no 
underlying 'signal' to amplify. For this reason, we investigate the utility of a recti- 
fying (or anti-Hebbian) rule. 
Figure 3: A schematic dis- 
play showing contingent positive 
and negative voltage curvatures 
(+i) above a segment of neu- 
ron, and inward and outward 
currents (5:i), through a par- 
ticular channel type. In situ- 
ations (a) and (b), a Hebbian 
version of Equation 3 will raise 
the channel density (} T), and 
in (c) and (d) an anti-Hebbian 
rule will do this. In the first two 
cases, the channels are polar- 
ising the membrane potential, 
creating high voltage curvature, 
while in the latter two, they are 
rectifying (or flattening) it. De- 
pending on the sign of e, equa- 
tion 3 attempts to either max- 
imise or minimise (O'V/Ox2) '. 
i k -re , 
' i +ve 
' , 
(b) ;k f if � is +ve 
, i k +ve : 
4 EXAMPLES 
For the purposes of demonstration, linear RLC electrical components are often used 
here. These simple 'intrinsic' (non-synaptic) components have the most tractable 
kinetics of any, and as shown by [11] and [9], the impedances they create capture 
some of the properties of active membrane. The components are leakage resistances, 
capacitances and inductances, whose y}'s are given by 1//, (7 and 1/L respectively. 
During learning, all }'s were kept above zero for reasons of stability. 
4.1 LEARNING RESONANCE 
In this experiment, an RLC 'compartment' with no frequency preference was stim- 
ulated at a certain frequency and trained according to equation 3 with e negative. 
After training, the frequency response curve of the circuit had a resonant peak at 
the training frequency (Figure 4). This result is significant since many auditory 
and tactile sensory cells are tuned to certain frequencies, and we know that a major 
component of the tuning is electrical, with resonances created by particular balances 
of ion channel populations [13]. 
Anti-Hebb in 'Channel Space'? 63 
! 
Figure 4: Learning resonance. The curves show the frequency-response curves of 
the compartment before and after training at a frequency of 0.4. 
4.2 LEARNING CONDUCTION 
Another role that intrinsic channels must play within a cell is the faithful transmis- 
sion of information. Any voltage curvatures at a point away from a synapse signify 
a net cross membrane current which can be seen as distorting the signal in the cable. 
Thus, by removing voltage curvatures, we preserve the signal. This is demonstrated 
Figure 5: Learning conduction. The cable consists of a chain of compartments, 
which only conduct the impulse after they acquire active channels. 
in the following example: 'learning to be an axon'. A non-linear spiking compart- 
ment with Morris-Lecar Calk kinetics (see [14]) is coupled to a long passive cable. 
Before learning, the signal decays passively in the cable (Figure 5). The driving 
compartment -vector, and the capacitances in the cable are then clamped to stop 
the system from converging on the null solution (  0). All other 's (including 
spiking conductduces in the cable) can then learn. The first thing learnt was that 
the inward and outward leakage conductduces (+ and -) adjusted themselves to 
make the average voltage curvature in each compartment zero (just as bias units in 
error correction algorithms adjust to make the 
