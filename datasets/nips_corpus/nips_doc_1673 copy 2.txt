Noisy Neural Networks and 
Generalizations 
Hava T. Siegelmon 
Industrial Eng. and Management, Mathematics 
Technion- IIT 
Haifa 32000, Israel 
iehava @ie. technion. ac. il 
Alexander Roitershtein 
Mathematics 
Technion- IIT 
Haifa 32000, Israel 
roiterst @math. technion. ac. il 
Asa Ben-Hur 
Industrial Eng. and Management 
Technion- IIT 
Haifa 32000, Israel 
asa @tx. tech nion. a c. il 
Abstract 
In this paper we define a probabilistic computational model which 
generalizes many noisy neural network models, including the recent 
work of Maass and Sontag [5]. We identify weak ergodic.ity as the 
mechanism responsible for restriction of the computational power 
of probabilistic models to definite languages, independent of the 
characteristics of the noise: whether it is discrete or analog, or if 
it depends on the input or not, and independent of whether the 
variables are discrete or continuous. We give examples of weakly 
ergodic models including noisy computational systems with noise 
depending on the current state and inputs, aggregate models, and 
computational systems which update in continuous time. 
I Introduction 
Noisy neural networks were recently examined, e.g. in. [1, 4, 5]. It was shown in [5] 
that Gaussian-like noise reduces the power of analog recurrent neural networks to 
the class of definite languages, which are a strict subset of regular languages. Let 
E be an arbitrary alphabet. L C E* is called a definite language if for some integer 
r any two words coinciding on the last r symbols are either both in L or neither in 
L. The ability of a computational system to recognize only definite languages can 
be interpreted as saying that the system forgets all its input signals, except for the 
most recent ones. This property is reminiscent of human short term memory. 
Definite probabilistic computational models have their roots in Rabin's pioneer- 
ing work on probabilistic automata [9]. He identified a condition on probabilistic 
automata with a finite state space which restricts them to definite languages. Paz 
[8] generalized Rabin's condition, applying it to automata with a countable state 
space, and calling it weak ergodicity [7, 8]. In their ground-breaking paper [5], 
336 H. T. Siegelmann, A. Roitershtein and A. Ben-Hur 
Maass and Sontag extended the principle leading to definite languages to a finite 
interconnection of continuous-valued neurons. They proved that in the presence 
of analog noise (e.g. Gaussian), recurrent neural networks are limited in their 
computational power to definite languages. Under a different noise model, Maass 
and Orponen [4] and Casey [1] showed that such neural networks are reduced in 
their power to regular languages. 
In this paper we generalize the condition of weak ergodicity, making it applica- 
ble to numerous probabilistic computational machines. In our general probabilistic 
model, the state space can be arbitrary: it is not constrained to be a finite or 
infinite set, to be a discrete or non-discrete subset of some Euclidean space, or 
even to be a metric or topological space. The input alphabet is arbitrary as well 
(e.g., bits, rationals, reals, etc.). The stochasticity is not necessarily defined via a 
transition probability function (TPF) as in all the aforementioned probabilistic and 
noisy models, but through the more general Markov operators acting on measures. 
Our Markov Computational Systems (MCS's) include as special cases Rabin's ac- 
tual probabilistic automata with cut-point [9], the quasi-definite automata by Paz 
[8], and the noisy analog neural network by Maass and Sontag [5]. Interestingly, 
our model also includes: analog dynamical systems and neural models, which have 
no underlying deterministic rule but rather update probabilistically by using finite 
memory; neural networks with an unbounded number of components; networks of 
variable dimension (e.g., recruiting networks); hybrid systems that combine dis- 
crete and continuous variables; stochastic cellular automata; and stochastic coupled 
map lattices. 
We prove that all weakly ergodic Markov systems are stable, i.e. are robust with 
respect to architectural imprecisions and environmental noise. This property is de- 
sirable for both biological and artificial neural networks. This robustness was known 
up to now only for the classical discrete probabilistic automata [8, 9]. To enable 
practicality and ease in deciding weak ergodicity for given systems, we provide two 
conditions on the transition probability functions under which the associated com- 
putational system becomes weakly ergodic. One condition is based on a version 
of Doeblin's condition [5] while the second is motivated by the theory of scram- 
bling matrices [7, 8]. In addition we construct various examples of weakly ergodic 
systems which include synchronous or asynchronous computational systems, and 
hybrid continuous and discrete time systems. 
2 Markov Computational System (MCS) 
Instead of describing various types of noisy neural network models or stochastic 
dynamical systems we define a general abstract probabilistic model. When dealing 
with systems containing inherent elements of uncertainty (e.g., noise) we abandon 
the study of individual trajectories in favor of an examination of the flow of state 
distributions. The noise models we consider are homogeneous in time, in that they 
may depend on the input, but do not depend on time. The dynamics we consider 
is defined by operators acting in the space of measures, and are called Markov 
operators [6]. In the following we define the concepts which are required for such 
an approach. 
Let E be an arbitrary alphabet and f2 be an abstract state space. We assume that 
a er-algebra B (not necessarily Borel sets) of subsets of f is given, thus (f, B) is a 
measurable space. Let us denote by P the set of probability measures on (f, 
This set is called a distribution space. 
Let � be a space of finite measures on (f, B) with the total variation norm defined 
Noisy Neural Networks and Generalizations 337 
by 
[1111 ----[1(') -- sup/(A)- inf/(A). (1) 
AB AB 
Denote by � the set of all bounded linear operators acting from � to itself. The 
I1' ]]1- norm on � induces a norm IIPI] - supe ]IP/11 in �. An operator P  � 
is said to be a Markov operator if for any probability measure/  P, the image P/ 
is again a probability measure. For a Markov operator, IIPII - 1. 
Definition 2.1 A Markov system is a set of Markov operators T = {P,: u  Z}. 
With any Markov system T, one can associate a probabilistic computational sys- 
tem. If the probability distribution on the initial states is given by the probability 
measure/0, then the distribution of states after n computational steps on inputs 
w - w0, Wl, ..., w, is defined as in [5, 8] 
Pw!o(A) - Pon '...' PoP0!o � (2) 
Let .A and 7 be two subset of 7 ) with the property of having a p-gap 
dist(A, ) -- inf lip- '1[  P > 0 (3) 
The first set is called a set of accepting distributions and the second is called a set 
of rejecting distributions. A language L 6 E* is said to be recognized by Markov 
computational system A = (�, A, 7, E,/0, T) if 
w 6 LP!o6A 
w q L,Poo7. 
This model of language recognition with a gap between accepting and rejecting 
spaces agrees with Rabin's model of probabilistic automata with isolated cut-point 
[9] and the model of analog probabilistic computation [4, 5]. 
An example of a Markov system is a system of operators defined by TPF on (, B). 
Let Pu (x, A) be the probability of moving from a state x to the set of states A upon 
receiving the input signal u  E. The function Pu(x, .) is a probability measure for 
all x   and Pu(', A) is a measurable function of x for any A  B. In this case, 
Pu!(A) are defined by 
P'!(A) =/c P,(x, A)!(dx). (4) 
3 Weakly Ergodic MCS 
1 
Let P 6 � be a Markov operator. The real number 7(P) = I -  sup,,e ]lPp - 
PYI[ is called the ergodicity coefficient of the Markov operator. We denote 
((P) = I- 7(P). It can be proven that for any two Markov operators P,P2, 
((PP2) _< ((P)J(P2). The ergodicity coefficient was introduced by Dobrushin [2] 
for the particular case of Markov operators induced by TPF P(x, A). In this special 
case 7(P) = 1 -supr,y sup A [P(x,A)- P(y,A)I. 
Weakly ergodic systems were introduced and studied by Paz in the particular case 
of a denumerable state space , where Markov operators are represented by infi- 
nite dimensional matrices. The following definition makes no assumption on the 
associated measurable space. 
Definition 3.1 A Markov system {P,, u G Z} is called weakly ergodic if for any 
c > 0, there is an integer r -- r(c 0 such that for any w G E >r and any p, v  P, 
1 
338 H.T. Siegelmann, ,4. Roitershtein and,4. Ben-Hur 
An MCS AA is called weakly ergodic if its associated Markov system 
is weakly ergodic. 
An MCS JM is weakly ergodic if and only if there is an integer r and real number 
c < 1, such that IIPw/ - Pwt, 111 _< c for any word w of length r. Our most general 
characterization of weak ergodicity is as follows: [11]: 
Theorem 1 An abstract MCS .M is weakly ergodic if and only if there exists 
a multiplicative operator's norm [l' II** on � equivalent to the norm I1' I[t :- 
liP;kill and such that suPuez IlP. II** < * for some number s < 1 � 
sup{ x:xn=0} IlXll ' - ' 
The next theorem connects the computational power of weakly ergodic MCS's with 
the class of definite languages, generalizing the results by Rabin [9], Paz [8, p. 175], 
and Maass and Sontag [5]. 
Theorem 2 Let .M be a weakly ergodic MCS. If a language L can be recognized by 
.M, then it is definite. � 
4 The Stability Theorem of Weakly Ergodic MCS 
An important issue for any computational system is whether the machine is robust 
with respect to small perturbations of the system's parameters or under some ex- 
ternal noise. The stability of language recognition by weakly ergodic M
