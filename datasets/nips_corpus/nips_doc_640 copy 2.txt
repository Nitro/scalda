Learning Curves, Model Selection and 
Complexity of Neural Networks 
Noboru Murata 
Departlnent of Mathematical Engineering and Information Physics 
University of rIbkyo, Tokyo 113, JAPAN 
E-mail: murasa;. t. u-tokyo. ac. jp 
Shuji Yoshizawa 
Dept. Mech. Info. 
University of Tokyo 
Shun-ichi Amari 
Dept. Math. Eng. and Info. 
University of Tokyo 
Phys. 
Abstract 
Learning curves show hoxv a neural network is improved as the 
number of training examples increases and how it is related to 
the network complexity. The present paper clarifies asymptotic 
properties and their relation of two learning curves, one concerning 
the predictive loss or generalization loss and the other the training 
loss. The result gives a natural definition of the complexity of a 
neural network. Moreover, it provides a new criterion of model 
selection. 
1 INTRODUCTION 
The learning curve shows hoxv well the behavior of a neural network is improved as 
the number of training exanq)les increases and how it is related with the complexity 
of neural networks. This provides us with a. criterion for choosing an adequate 
network in relation to the number 
of training examples. Some researchers have attacked this problem by using sta- 
tistical mechanical methods (see Levin et al. [1990], Seung et al. [1991], etc.) 
and some by information theory and algorithmic inethods (see Baum and Haussler 
607 
608 Murata, Yoshizawa, and Amari 
[1989], etc.). The present paper elucidates asymptotic properties of the learning 
curve from the statistical point of view, giving a new criterion for model selection. 
2 STATEMENT OF THE PROBLEM 
Let us consider a stochastic neural network, which is parameterized by a set of rn 
weights 0 = (01, ... ,0 ') and whose input-output relation is specified by a condi- 
tional probability p(yl x, 0). In other words, for an input signal is x � R ', the 
probability distribution of output y �It '�u' is given by p(yl x, 0). 
A typical form of the stochastic neural network is as follows: let us consider a 
multi-layered network fix, O) where 0 is aset of m parameters 0 = (01 , ...,0 m) and 
its components correspond to weights and thresholds of the network. When some 
input x is given, the network produce an output 
y = fix,O) + l(x), (1) 
where r/(x) is noise whose conditional distribution is given by a(r/]x). Then the 
conditional distribution of the network, which specifies the input-output relation, 
is given by 
p(y[x,O) = a(y- f(x,O)]x). (2) 
We define a training sample t _ {(x,yx),...,(x,,y,)) as a set of t examples 
generated fi'om the true conditional distribution q(ylx), where xi is generated from 
a probability distribution r(x) independently. We should note that both r(x) and 
q(ylx) are unknown and we need not assume the faithfulness of the model, that is, 
we do not assume that there exists a parameter O* which realize the true distribution 
q(ylx) such that p(ylx,O*) = q(ylx). 
Our purpose is to find an appropriate parameter 0 which realizes a good approxi- 
mation p(y[x, O) to q(y[x). For this purpose, we use a loss function 
L(O) = D(r;qlp(O)) + S(O) (3) 
a.s a criterion to bc minimized, where D(r; q[p(O)) represents a general divergence 
measure between two conditional probabilities q(yx) and p(y]x, O) in the expecta- 
t. ion form under the true input-output probability 
D(r;q]p(O)) = f r(x)q(y]x)k(x,y,O)dxdy (4) 
and S(O) is a regularization term to fit the smoothness condition of outputs (Moody 
[1992]). So the loss function is rewritten as a expectation tbrm 
L(O) = f d(x,y,O) = k(x,y,O) + S(O), (5) 
and d(x, y, 0) is callcd the pointwise loss filllCt. iOll. 
A typical case of the divergence D of the multi-layered network f(x, O) with noise 
is the squared error 
D(r;q[p(O)) = f r(x)q(y[x)][y- f(x,O)l]udxdy, (6) 
Learning Curves, Model Selection and Complexity of Neural Networks 609 
The error function of an ordinary multi-layered network is in this form, and the 
conventional Back-Propagation method is derived from this type of loss function. 
Another typical case is the Kullback-Leibler divergence 
m(,';q[p(O)) = f ,'(x)q(ylx)log q(ylx) 
p(ylx,O)dxlv. (7) 
The integration f r(z)q(ylz)logq(yl')(xdy is a constant called a conditional en- 
tropy, and we usually use the following abbreviated form instead of the previous 
divergence: 
D(r; qlp(O)) - - / r(x)q(ylx) log p(y]x, O)dxdy. (8) 
Next, we define an optimum of the parameter in the sense of the loss function that 
we introduced. We denote by 0* the optimal parameter that minimizes the loss 
function L(O), that is, 
L(O*) = min L(0), (9) 
0 
and we regard p(yl x, 0') as the best realization of the model. 
When a training sample t is given, we can also define an clnpirical loss function: 
.(0): r)(,'; ql,(0)) + 3(0), (10) 
where i , 0 are the empirical distributions given by the sample ct, that is, 
t 
1 
e (11) 
i=1 
In practical case, we consider the empirical loss function and search for the quasi- 
optimal parameter 0 defined by 
�(0): �(0), 
0 
because the true distributions r(x') and q(ylz) are unknown and we can only use 
exanples (.l:i,.qi) obserwed fi'on the [rue distribution r(.)q([). We should note 
that the quasi-ol>timl parancter 0 is a random variable depending on the sample 
t, each element of which is chosen randomly. 
The following lemma guarantees that we can use the empirical loss function instead 
of the actual loss function when the number of examples t is large. 
Lemma 1 If the umbcr of escamplcs t is large enough, it is shown that the quasi- 
optimal parameter 0 is 7ormally distributed around the optimal parameter 0', that 
is, 
 O , (13) 
(.7 = r(x)q(ylx)Wd(x,y.O*)Wd(x,y,O*)q'dzdy, 
j. 
Q : r(x)q(ylx)VWd(x,v,O*)ddv, 
and W denoles the d(lferenhal operator wilh respect to O. 
(14) 
(15) 
610 Murata, Yoshizawa, and Amari 
This lemma is proved by using the usual statistical methods. 
3 LEARNING PROCEDURE 
In many cases, however, it is difficult to obtain tile quasi-optimal parameter t by 
minimizing the equation (10) directly. We theretbre often use a stochastic descent 
method to get an approximation to the quasi-optimal parameter t. 
Definition 1 (Stochastic Descent Method) In each learning step, an example 
is re-sampled from the gi'vc sample t randomly, and the following modification is 
applied to the parameter On at step n, 
0,,+ = O, - cVd(xi(,), Yi(n), On), (16) 
where  is a positive value called a learnig coefficient and (xi(,), Yi(n)) is the re- 
sampled example at step n. 
This is a sequential learning method and the operations of random sampling 
fi'om (t in each learning step is called the re-sampling plan. The parameter 
O, at step , is a randon variable as a function of the re-sampled sequence 
w = {(a'il 1, .qill),', (.vit,, I, Yit,, I)}' However, if the initial value of 0 is appropriate 
(this assunaption prevents being stuck in local minima) and if the learning step n 
is large enough, it. is shown that the learned parameter 0, is normally distributed 
around the qnasi-optimal parameter. 
Lemma 2 If the learning step n is large enough and the learning coefficient e is 
small enough, the parameter 0,. is normally distributed asymptotically, that is, 
O... N(O, cl/), (17) 
wb. cre I/ satisfies the followig rclatim 
G: QV + VQ, 
t t 
l ' 
i=1 i=1 
77d( 3:i, yi , 0). 
(18) 
In tile IBlloving discussion, we assume that , is large enough and c is small enough, 
and we denote the learned parameter by 
0,,). (19) 
The distribution of the randon variable , theretbre, can be regarded as the normal 
distrii>ution N(O, eV). 
4 LEARNING CURVES 
It. is important, to evalua[c the diflbrence between two quantities L(t) and (t). The 
quanlity L(t) is called the predictive loss or the generalization error, which shows 
Learning Curves, Model Selection and Complexity of Neural Networks 611 
the average loss of the trained net.vork when a novel example is given. On the other 
hand, tile quantity L()is called the training loss or the training error, which shows 
the average loss evaluated by tile examples used in training. Since these quantities 
depend on the sample sct and the re-sampled sequence w', we take the expectation 
E and the variance Vat with respect to tile sample sct and the re-sampling sequence 
First, let us consider the predictive loss which is tile average loss of the trained 
network when a new example (which does not belong to the sample sct) is given. 
This averaging operation is replaced by averaging all over the input-output pairs, 
because the measure of the sample sct is zero. Then the predictive loss is written as 
L((}) - J r(x)q(ylx)a(x,y,)dxay. (20) 
From the properties of  and , we can prove the following important relations. 
Theorem 1 Tb.e predictive loss asymptotically satisfies 
E[L(t)] : L(O*) +. t,.C:O - + trOV, 
?- 
1 trC;O_C;Q_ +troVQV+ trGV. 
Var[L()]- 2t   
(2) 
(22) 
Roughly speaking, there exist two rado values Y1 and YS, and the predictive loss 
can be xvrilten as the lb!lowing tbl'n: 
I.() = L(O*)+ trGO -1 + t,-Ok 
+ s': + %7) + op�), (2a) 
where }5 mad } satisfy 
-g[�]: o, 
I , GQ_  
Va,-[Y] = :trOO -  
1 
,;[>.:_,]: 0, Varig':_,]: tt'Ovov, 
Cov[Sq:_,] = trGV, 
E, \;at and Cov denote tim exl>ectal. ion, the variance and the covariance respectively. 
Next, we consider tim training loss, i.e., the average loss evaluated by the examples 
used in training. Just as we did in the previous theorem, we can get the following 
relations. 
Theorem 2 
The trai.iu9 loss asymptotically satisfy 
1 t, rGQ_ 1 + trQV, 
= (o*)- 2-7 
: 7. ,.(.)q(l.c)d(.r,,O*)-d:,.d 
(24) 
(25) 
612 Murata, Yoshizawa, and Amari 
Intuitively speaking like the predictive loss, the training loss can be expanded as 
L(O*)- trGQ -x + trQV 
+ + 
where Ya satisfies 
- o, 
VatlYe] - / r(x)q(ylx)d(x, 
(2s) 
Y'O')2dxdy - (/r(x)q(y[x)d(x'Y'O*)dxdy)  
When we look at two curves E[L()] and E[L()] as functions of t, they are called 
learni
